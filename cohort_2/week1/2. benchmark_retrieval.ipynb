{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1 : Systematically Improving Your RAG Application\n",
    "\n",
    "## Benchmarking Retrieval Methods\n",
    "\n",
    "> If you have not already, please run the `1. synthetic_questions.ipynb` notebook to generate the synthetic questions we'll be using in this notebook before proceeding.\n",
    "\n",
    "After generating the synthetic questions, we'll use them to benchmark the recall and mrr of various retrievel methods. We want to do so because these metrics give us an objective measure of how well a retrieval method is performing.\n",
    "\n",
    "This allows us to make an informed decision about whether a retrieval method might be worth the cost and latency to implement. For instance, if we get a 1% improvement in recall@10 but latency increase by 10x, we might not want to use that method.\n",
    "\n",
    "We'll do this in two steps\n",
    "\n",
    "1. First, we'll take our original dataset of sql snippets from `567-labs/bird-rag` and ingest it into a local lancedb instance.\n",
    "2. Then we'll show you how to measure recall and mrr at different levels of k\n",
    "\n",
    "Recall that in our original dataset, each row had the following columns\n",
    "\n",
    "- `id`: The unique identifier for each row\n",
    "- `query`: The SQL snippet\n",
    "- `difficulty`: The difficulty of the question\n",
    "\n",
    "When we query our database with a synthetic question, we'll retrieve a list of chunks and their chunk ids that match the query. Our goal here is to verify that we're able to retrieve the correct chunk id for each question. \n",
    "\n",
    "We'll be using `braintrust` to collect the data and discuss the trade offs of each method. We like `braintrust` because it allows us to easily run multiple experiments using a simple `Eval` object and share the results easily if you're working with a team.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Our Rag Pipeline\n",
    "\n",
    "In this example, we're using a local `lancedb` instance. We're doing so because of 3 reasons.\n",
    "\n",
    "1. LanceDB handles the embeddings of our data for us \n",
    "2. It provides embedding search, hybrid search and other re-ranking methods all within a single api.\n",
    "3. We can use Pydantic to define our table schema and easily ingest our data.\n",
    "\n",
    "This makes it quick and easy for us to compare the performance of each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1528 chunks ingested into the database\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import lancedb\n",
    "from lancedb.pydantic import LanceModel, Vector\n",
    "from lancedb.embeddings import get_registry\n",
    "\n",
    "# Create LanceDB Instance\n",
    "db = lancedb.connect(\"./lancedb\")\n",
    "\n",
    "# Define and create Table using Pydantic\n",
    "func = get_registry().get(\"openai\").create(name=\"text-embedding-3-small\")\n",
    "\n",
    "\n",
    "class Chunk(LanceModel):\n",
    "    id: str\n",
    "    query: str = func.SourceField()\n",
    "    vector: Vector(func.ndims()) = func.VectorField()\n",
    "\n",
    "\n",
    "table = db.create_table(\"chunks\", schema=Chunk, mode=\"overwrite\")\n",
    "\n",
    "# Ingest dataset into table\n",
    "dataset = datasets.load_dataset(\"567-labs/bird-rag\")[\"train\"]\n",
    "formatted_dataset = [{\"id\": item[\"id\"], \"query\": item[\"query\"]} for item in dataset]\n",
    "\n",
    "table.add(formatted_dataset)\n",
    "\n",
    "table.create_fts_index(\"query\", replace=True)\n",
    "print(f\"{table.count_rows()} chunks ingested into the database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Metrics\n",
    "\n",
    "Let's now start by evaluating the retrieval performance of our model. We'll do so by measuring the recall and MRR at different levels of k.\n",
    "\n",
    "$$ \\text{Recall} = \\frac{\\text{Number of Relevant Items Retrieved}}{\\text{Total Number of Relevant Items}} $$ \n",
    "\n",
    "$$ \\text{MRR} = \\frac{\\sum_{i=1}^{n} \\frac{1}{rank(i)}}{n} $$ \n",
    "\n",
    "As models improve, their context window and reasoning abilities improve. This means that their ability to select relevant information in response to a user query will improve. By optimizing for recall, we ensure that the language model has access to all necessary information, which can lead to more accurate and reliable generated responses.\n",
    "\n",
    "MRR@K is a useful metric if we want to display retrieved results as citations to users. We normally show a smaller list of retrieved results to users and we want to make sure that the correct result is ranked highly during retrieval so that it's more likely to be selected.\n",
    "\n",
    "<TODO: Jason please verify or add on here >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mrr(predictions: list[str], gt: list[str]):\n",
    "    mrr = 0\n",
    "    for label in gt:\n",
    "        if label in predictions:\n",
    "            # Find the relevant item that has the smallest index\n",
    "            mrr = max(mrr, 1 / (predictions.index(label) + 1))\n",
    "    return mrr\n",
    "\n",
    "\n",
    "def calculate_recall(predictions: list[str], gt: list[str]):\n",
    "    # Calculate the proportion of relevant items that were retrieved\n",
    "    return len([label for label in gt if label in predictions]) / len(gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Metrics\n",
    "\n",
    "We want to determine whether we can retrieve the relevant SQL snippet for each synthetic question. We can do so by querying our LanceDB instance. Since the `search` api provides easy support for re-rankers and different search methods, using it allows us to quickly test out different retrieval methods.\n",
    "\n",
    "\n",
    "Remember that in our earlier notebook, we generated the question below for the corresponding SQL snippet.\n",
    "\n",
    "Question : Which schools in France are locally funded and have a greater difference between their total K-12 \n",
    "enrollment and enrollment for ages 5-17 compared to the average difference for locally funded schools?\n",
    "\n",
    "Snippet \n",
    "```sql\n",
    "SELECT T2.School, T2.DOC FROM frpm AS T1 INNER JOIN schools AS T2 ON T1.CDSCode = T2.CDSCode WHERE\n",
    "T2.FundingType = 'Locally funded' AND (T1.`Enrollment (K-12)` - T1.`Enrollment (Ages 5-17)`) > (SELECT \n",
    "AVG(T3.`Enrollment (K-12)` - T3.`Enrollment (Ages 5-17)`) FROM frpm AS T3 INNER JOIN schools AS T4 ON T3.CDSCode = \n",
    "T4.CDSCode WHERE T4.FundingType = 'Locally funded')\n",
    "```\n",
    "\n",
    "Let's see this in action by fetching the top 25 results from our database that match this query using embedding search. Notice here that we're using the chunk id to calculate recall and mrr.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">28</span> SELECT T2.School, T2.DOC FROM frpm AS T1 INNER JOIN schools AS T2 ON T1.CDSCode = T2.CDSCode WHERE \n",
       "T2.FundingType = <span style=\"color: #008000; text-decoration-color: #008000\">'Locally funded'</span> AND <span style=\"font-weight: bold\">(</span>T1.`Enrollment <span style=\"font-weight: bold\">(</span>K-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span><span style=\"font-weight: bold\">)</span>` - T1.`Enrollment <span style=\"font-weight: bold\">(</span>Ages <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span><span style=\"font-weight: bold\">)</span>`<span style=\"font-weight: bold\">)</span> &gt; <span style=\"font-weight: bold\">(</span>SELECT \n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AVG</span><span style=\"font-weight: bold\">(</span>T3.`Enrollment <span style=\"font-weight: bold\">(</span>K-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span><span style=\"font-weight: bold\">)</span>` - T3.`Enrollment <span style=\"font-weight: bold\">(</span>Ages <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span><span style=\"font-weight: bold\">)</span>`<span style=\"font-weight: bold\">)</span> FROM frpm AS T3 INNER JOIN schools AS T4 ON T3.CDSCode = \n",
       "T4.CDSCode WHERE T4.FundingType = <span style=\"color: #008000; text-decoration-color: #008000\">'Locally funded'</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m28\u001b[0m SELECT T2.School, T2.DOC FROM frpm AS T1 INNER JOIN schools AS T2 ON T1.CDSCode = T2.CDSCode WHERE \n",
       "T2.FundingType = \u001b[32m'Locally funded'\u001b[0m AND \u001b[1m(\u001b[0mT1.`Enrollment \u001b[1m(\u001b[0mK-\u001b[1;36m12\u001b[0m\u001b[1m)\u001b[0m` - T1.`Enrollment \u001b[1m(\u001b[0mAges \u001b[1;36m5\u001b[0m-\u001b[1;36m17\u001b[0m\u001b[1m)\u001b[0m`\u001b[1m)\u001b[0m > \u001b[1m(\u001b[0mSELECT \n",
       "\u001b[1;35mAVG\u001b[0m\u001b[1m(\u001b[0mT3.`Enrollment \u001b[1m(\u001b[0mK-\u001b[1;36m12\u001b[0m\u001b[1m)\u001b[0m` - T3.`Enrollment \u001b[1m(\u001b[0mAges \u001b[1;36m5\u001b[0m-\u001b[1;36m17\u001b[0m\u001b[1m)\u001b[0m`\u001b[1m)\u001b[0m FROM frpm AS T3 INNER JOIN schools AS T4 ON T3.CDSCode = \n",
       "T4.CDSCode WHERE T4.FundingType = \u001b[32m'Locally funded'\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span> SELECT T1.NCESSchool FROM schools AS T1 INNER JOIN frpm AS T2 ON T1.CDSCode = T2.CDSCode ORDER BY T2.`Enrollment\n",
       "<span style=\"font-weight: bold\">(</span>Ages <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span><span style=\"font-weight: bold\">)</span>` DESC LIMIT <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m14\u001b[0m SELECT T1.NCESSchool FROM schools AS T1 INNER JOIN frpm AS T2 ON T1.CDSCode = T2.CDSCode ORDER BY T2.`Enrollment\n",
       "\u001b[1m(\u001b[0mAges \u001b[1;36m5\u001b[0m-\u001b[1;36m17\u001b[0m\u001b[1m)\u001b[0m` DESC LIMIT \u001b[1;36m5\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich import print\n",
    "\n",
    "question = \"Which schools in France are locally funded and have a greater difference between their total K-12 enrollment and enrollment for ages 5-17 compared to the average difference for locally funded schools?\"\n",
    "\n",
    "query = \"SELECT T2.School, T2.DOC FROM frpm AS T1 INNER JOIN schools AS T2 ON T1.CDSCode = T2.CDSCode WHERE T2.FundingType = 'Locally funded' AND (T1.`Enrollment (K-12)` - T1.`Enrollment (Ages 5-17)`) > (SELECT AVG(T3.`Enrollment (K-12)` - T3.`Enrollment (Ages 5-17)`) FROM frpm AS T3 INNER JOIN schools AS T4 ON T3.CDSCode = T4.CDSCode WHERE T4.FundingType = 'Locally funded')\"\n",
    "\n",
    "retrieved_items = table.search(question).limit(25).to_list()\n",
    "\n",
    "for item in retrieved_items[:2]:\n",
    "    print(item[\"id\"], item[\"query\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the first item is a perfect match for our desired chunk. We're going to calculate the recall@25 and MRR@25 for this specific snippet. To do so, we'll use its id. The id of this specific snippet is 28. You can verify it by looking at the filtered subset of the hugging face dataset [here](https://huggingface.co/datasets/567-labs/bird-rag/viewer/default/train?q=T4.FundingType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">MRR@<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "MRR@\u001b[1;36m25\u001b[0m: \u001b[1;36m1.0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Recall@<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Recall@\u001b[1;36m25\u001b[0m: \u001b[1;36m1.0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Relevant item found at rank: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Relevant item found at rank: \u001b[1;36m1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predicted_ids = [item[\"id\"] for item in retrieved_items]\n",
    "ground_truth_ids = [\"28\"]  # The ID of the expected SQL snippet\n",
    "\n",
    "# Calculate metrics\n",
    "mrr_score = calculate_mrr(predicted_ids, ground_truth_ids)\n",
    "recall_score = calculate_recall(predicted_ids, ground_truth_ids)\n",
    "rank_position = predicted_ids.index(\"28\") + 1  # Adding 1 because index starts at 0\n",
    "\n",
    "print(f\"MRR@25: {mrr_score}\")\n",
    "print(f\"Recall@25: {recall_score}\")\n",
    "print(f\"Relevant item found at rank: {rank_position}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple query since we have many of the specific column values in the snippet listed in the question - Eg. (K-12) and (Ages 5-17). But we're able to see how well our retrieval system is performing in this case with an objective measure.\n",
    "\n",
    "We use a wrapper to compute metrics for multiple subsets of the retrieved items without having to hard code functions that take in a subset of the retrieved items. This makes it easy to vary the size of k and the metrics we want to use instead of hard coding them.\n",
    "\n",
    "<TODO: Jason please verify this>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the names for our metrics\n",
    "eval_metrics = [[\"mrr\", calculate_mrr], [\"recall\", calculate_recall]]\n",
    "\n",
    "# Define the different sizes of k that we want to compute the metrics at\n",
    "sizes = [1, 3, 5, 10]\n",
    "\n",
    "# Define wrapper functions that will take a subset of k items and calculate the relevant metric\n",
    "metrics = {}\n",
    "for metric_name, metric_fn in eval_metrics:\n",
    "    for size in sizes:\n",
    "        key = f\"{metric_name}@{size}\"\n",
    "        metrics[key] = lambda predictions, gt, m=metric_fn, s=size: m(\n",
    "            predictions[:s], gt\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see this in action below with a simple example.\n",
    "\n",
    "Our MRR@1 and Recall@1 will be 0 since there are no relevant items at k = 1. However, at k=3, we see that we have `a` in the top 3 retrieved items. This gives us a MRR@3 of 1/3 and a Recall@3 of 1/2 ( since we retrieved 1 of 2 relevant items).\n",
    "\n",
    "Once we look at the MRR@5, we see that we have `a` and `b` in the top 5 retrieved items. This gives us a MRR@5 of 1/3 and a Recall@5 of 2/2 (since we retrieved all of the relevant items). We can see that we have the same values for MRR@10 and Recall@10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mrr@1': 0,\n",
       " 'mrr@3': 0.3333333333333333,\n",
       " 'mrr@5': 0.3333333333333333,\n",
       " 'mrr@10': 0.3333333333333333,\n",
       " 'recall@1': 0.0,\n",
       " 'recall@3': 0.5,\n",
       " 'recall@5': 1.0,\n",
       " 'recall@10': 1.0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [\"a\", \"b\"]\n",
    "preds = [\"x\", \"y\", \"a\", \"c\", \"b\"]\n",
    "\n",
    "{metric: score_fn(preds, labels) for metric, score_fn in metrics.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running your first Benchmark\n",
    "\n",
    "We use braintrust here because it's easy to setup complex experiments. We can use the `Eval` object to define a single experiment and run it with different configurations. \n",
    "\n",
    "We can do so by defining a \n",
    "\n",
    "- `Task` : This is a function that takes in the input and returns an expected output\n",
    "- `Scorer` : This is a function that looks at the output and compares it to the expected output and returns a score. In our case, since we're comparing the recall and mrr at different levels of k, we'll be returning a list of scores.\n",
    "\n",
    "Since our task object is going to be running the retrieval with different methods, we'll also be redefining our `retrieve` function to take in additional parameters to be reconfigured for each experiment.\n",
    "\n",
    "We implemented (1) in the previous section. Now, we want to use a single function to retrieve the top k items from our database with different retrieval methods because it's significantly easier to modify a single function with configuration parameters than to keep track of individual functions for each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from braintrust import Score\n",
    "from lancedb.rerankers import CohereReranker\n",
    "import lancedb\n",
    "from typing import Literal\n",
    "\n",
    "db = lancedb.connect(\"./lancedb\")\n",
    "table = db.open_table(\"chunks\")\n",
    "# Configure a cohere reranker\n",
    "reranker = CohereReranker(model_name=\"rerank-multilingual-v3.0\", column=\"query\")\n",
    "\n",
    "\n",
    "def retrieve(\n",
    "    question: str,\n",
    "    max_k=25,\n",
    "    mode: Literal[\"vector\", \"fts\", \"hybrid\"] = \"vector\",\n",
    "    use_reranker: bool = False,\n",
    "):\n",
    "    results = table.search(question, query_type=mode).limit(max_k)\n",
    "    if use_reranker:\n",
    "        results = results.rerank(reranker=reranker)\n",
    "    return [result for result in results.to_list()]\n",
    "\n",
    "\n",
    "# Similar to our previous section, we can use the id of each item to compute the recall and MRR metrics.\n",
    "def evaluate_braintrust(input, output, **kwargs):\n",
    "    predictions = [item[\"id\"] for item in output]\n",
    "    labels = [kwargs[\"metadata\"][\"chunk_id\"]]\n",
    "    return [\n",
    "        Score(\n",
    "            name=metric,\n",
    "            score=score_fn(predictions, labels),\n",
    "            metadata={\"query\": input, \"result\": output, **kwargs[\"metadata\"]},\n",
    "        )\n",
    "        for metric, score_fn in metrics.items()\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Need to wait for braintrust to fix the issue with their api before I can run this\n",
    "\n",
    "from itertools import product\n",
    "from braintrust import Eval, init_dataset\n",
    "\n",
    "modes = [\"vector\", \"fts\", \"hybrid\"]\n",
    "reranker = [True, False]\n",
    "\n",
    "results = {}\n",
    "for mode, use_reranker in product(modes, reranker):\n",
    "    results[(mode, use_reranker)] = await Eval(\n",
    "        \"Text-2-SQL\",\n",
    "        data=init_dataset(project=\"Text-2-SQL\", name=\"Bird-Bench-Questions\"),\n",
    "        task=lambda input: retrieve(input, 25, mode=mode, use_reranker=use_reranker),\n",
    "        scores=[evaluate_braintrust],\n",
    "    )\n",
    "\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
