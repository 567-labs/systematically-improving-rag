{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook, we'll look at benchmarking the retrieval performance of embedding search given a user query. We'll be using the synthetic questions that we generated in the previous notebook to do so.\n",
    "\n",
    "We'll do so in two steps\n",
    "\n",
    "1. First, we'll take the same dataset that we used previously to generate the synthetic questions and ingest it into a local lancedb instance\n",
    "2. Then we'll show you how to measure recall and MRR at different levels of k for our synthetic questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion\n",
    "\n",
    "We'll start by using `lancedb` to ingest the dataset into a local database. This integrates nicely with `Pydantic` and also handles embedding and batching of of all of our data. Let's start by creating a new lancedb database and defining our Pydantic model for the chunks.\n",
    "\n",
    "Note here that we're using the `LanceModel` class from `lancedb` to define our Pydantic Model. This is a subclass of the BaseModel that adds additional functionality for working with LanceDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lancedb\n",
    "from lancedb.pydantic import LanceModel, Vector\n",
    "from lancedb.embeddings import get_registry\n",
    "\n",
    "db = lancedb.connect(\"./lancedb\")\n",
    "\n",
    "func = get_registry().get(\"openai\").create(name=\"text-embedding-3-small\")\n",
    "\n",
    "class Chunk(LanceModel):\n",
    "    id:str\n",
    "    text: str = func.SourceField()\n",
    "    vector: Vector(func.ndims()) = func.VectorField()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've defined our model and created our database. Let's start by ingesting our dataset into the database. We'll define a function to automatically batch our data, create a new table using the Pydantic model above and then ingest all of our chunks into the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import hashlib\n",
    "\n",
    "\n",
    "def batch_items(items:list[Chunk],batch_size:int=100):\n",
    "    cur = []\n",
    "    for item in items:\n",
    "        cur.append(item)\n",
    "        if len(cur) == batch_size:\n",
    "            yield cur\n",
    "            cur = []\n",
    "    if cur:\n",
    "        yield cur\n",
    "\n",
    "def hash_query(query:str) -> str:\n",
    "    return hashlib.sha256(query.encode()).hexdigest()\n",
    "    \n",
    "dataset = datasets.load_dataset(\"567-labs/bird-dev-snippets\")\n",
    "formatted_dataset = [\n",
    "    {\n",
    "        \"text\":item[\"query\"],\n",
    "        \"id\":hash_query(item[\"query\"])\n",
    "    }\n",
    "    for item in dataset[\"original\"]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created our batches, we can ingest them into the databse. We'll do so by creating a new table and then inserting each batch into the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:13,  2.29s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "table = db.create_table(\"chunks\",schema=Chunk,mode=\"overwrite\")\n",
    "\n",
    "batches = batch_items(formatted_dataset,300)\n",
    "\n",
    "for batch in tqdm(batches):\n",
    "    table.add(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Let's now start by evaluating the retrieval performance of our model. We'll do so by measuring the recall and MRR at different levels of k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mrr(predictions: list[str], gt: list[str]):\n",
    "    mrr = 0\n",
    "    for label in gt:\n",
    "        if label in predictions:\n",
    "            mrr = max(mrr, 1 / (predictions.index(label) + 1))\n",
    "    return mrr\n",
    "\n",
    "\n",
    "def calculate_recall(predictions: list[str], gt: list[str]):\n",
    "    return len([label for label in gt if label in predictions]) / len(gt)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Braintrust\n",
    "\n",
    "Now let's see how we can compute these using braintrust and lancedb. Let's start by writing a function that will take a query and return the top k results from our database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"SELECT T1.driverId FROM lapTimes AS T1 INNER JOIN races AS T2 on T1.raceId = T2.raceId WHERE T2.name = 'French Grand Prix' AND T1.lap = 3 ORDER BY T1.time DESC LIMIT 1\",\n",
       " 'SELECT T2.City FROM frpm AS T1 INNER JOIN schools AS T2 ON T1.CDSCode = T2.CDSCode GROUP BY T2.City ORDER BY SUM(T1.`Enrollment (K-12)`) ASC LIMIT 5']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def retrieve(query:str,k:int=10) -> list[str]:\n",
    "    results = table.search(query).limit(k).to_list()\n",
    "    return [result[\"text\"] for result in results]\n",
    "\n",
    "retrieve(\"What is the capital of France?\")[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define a simple list of functions that will take in these returned results and compute the recall and MRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "eval_metrics = [[\"mrr\", calculate_mrr], [\"recall\", calculate_recall]]\n",
    "sizes = [3, 5, 10, 15, 25]\n",
    "\n",
    "metrics = {\n",
    "    f\"{metric_name}@{size}\": lambda predictions, gt, m=metric_fn, s=size: (\n",
    "        lambda p, g: m(p[:s], g)\n",
    "    )(predictions, gt)\n",
    "    for (metric_name, metric_fn), size in itertools.product(eval_metrics, sizes)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment add-braintrust-support-1729509406 is running at https://www.braintrust.dev/app/567/p/Retrieval/experiments/add-braintrust-support-1729509406\n",
      "Retrieval (data): 145it [00:01, 116.71it/s]\n",
      "Retrieval (tasks): 100%|██████████| 145/145 [00:09<00:00, 15.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================SUMMARY=========================\n",
      "add-braintrust-support-1729509406 compared to add-braintrust-support-1729509191:\n",
      "76.78% (-01.93%) 'mrr@3'     score\t(0 improvements, 20 regressions)\n",
      "77.57% (-01.13%) 'mrr@5'     score\t(0 improvements, 15 regressions)\n",
      "78.37% (-00.34%) 'mrr@10'    score\t(0 improvements, 6 regressions)\n",
      "78.66% (-00.04%) 'mrr@15'    score\t(0 improvements, 1 regressions)\n",
      "78.71% (-) 'mrr@25'    score\t(0 improvements, 0 regressions)\n",
      "86.21% (-13.79%) 'recall@3'  score\t(0 improvements, 20 regressions)\n",
      "89.66% (-10.34%) 'recall@5'  score\t(0 improvements, 15 regressions)\n",
      "95.86% (-04.14%) 'recall@10' score\t(0 improvements, 6 regressions)\n",
      "99.31% (-00.69%) 'recall@15' score\t(0 improvements, 1 regressions)\n",
      "100.00% (-) 'recall@25' score\t(0 improvements, 0 regressions)\n",
      "\n",
      "4.53s (-92.24%) 'duration'\t(119 improvements, 26 regressions)\n",
      "\n",
      "See results for add-braintrust-support-1729509406 at https://www.braintrust.dev/app/567/p/Retrieval/experiments/add-braintrust-support-1729509406\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EvalResultWithSummary(summary=\"...\", results=[...])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from braintrust import Score, init_dataset, Eval\n",
    "\n",
    "def evaluate_braintrust(input, output, **kwargs):\n",
    "    hashed_queries = [hash_query(query) for query in output]\n",
    "    hashed_expected = [hash_query(query) for query in kwargs[\"expected\"]]\n",
    "    return [\n",
    "        Score(\n",
    "            name=metric,\n",
    "            score=score_fn(hashed_queries,hashed_expected),\n",
    "            metadata={\"query\": input, \"result\": output, **kwargs[\"metadata\"]},\n",
    "        )\n",
    "        for metric, score_fn in metrics.items()\n",
    "        \n",
    "    ]\n",
    "\n",
    "await Eval(\n",
    "    \"Retrieval\",\n",
    "    data=init_dataset(project=\"Retrieval\", name=\"Synthetic Questions\"),\n",
    "\n",
    "    task=lambda input: retrieve(input,25),\n",
    "    scores=[evaluate_braintrust],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
