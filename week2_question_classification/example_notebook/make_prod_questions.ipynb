{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context\n",
    "\n",
    "After releasing your RAG-based product, you will want to characterize the types of questions you receive and how you perform on each cluster of questions. This notebook generates synthetic data that is a stand-in for your production data. You won't use code from this notebook, and instead it creates assets used in analyze_clusters (which is a notebook you can reuse in your work.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code\n",
    "\n",
    "## Question Type Data\n",
    "\n",
    "Classification code won't have access to the true data generating process for the frequency of each question or the fraction that have thumbs up. So we separate them from other data in `question_types.py`. The frequency and satisfaction data is here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from question_types import QuestionTypes\n",
    "\n",
    "# Recency factor creates a temporal trend. Numbers <1 cause trend to more recent queries\n",
    "question_type_stats = {\n",
    "    QuestionTypes.COMPARISON: {\n",
    "        \"avg_questions_per_item\": 7,\n",
    "        \"frac_thumbs_up\": 0.1,\n",
    "        \"recency_factor\": 1,\n",
    "    },\n",
    "    QuestionTypes.VAGUE: {\n",
    "        \"avg_questions_per_item\": 5,\n",
    "        \"frac_thumbs_up\": 0.7,\n",
    "        \"recency_factor\": 1,\n",
    "    },\n",
    "    QuestionTypes.TYPICAL_PRICE: {\n",
    "        \"avg_questions_per_item\": 1,\n",
    "        \"frac_thumbs_up\": 0.3,\n",
    "        \"recency_factor\": 2,\n",
    "    },\n",
    "    QuestionTypes.CUSTOMER_SERVICE: {\n",
    "        \"avg_questions_per_item\": 2,\n",
    "        \"frac_thumbs_up\": 0.7,\n",
    "        \"recency_factor\": 1,\n",
    "    },\n",
    "    QuestionTypes.VISUAL: {\n",
    "        \"avg_questions_per_item\": 2,\n",
    "        \"frac_thumbs_up\": 0.1,\n",
    "        \"recency_factor\": 1,\n",
    "    },\n",
    "    QuestionTypes.ACCESSORIES: {\n",
    "        \"avg_questions_per_item\": 2,\n",
    "        \"frac_thumbs_up\": 0.3,\n",
    "        \"recency_factor\": 1,\n",
    "    },\n",
    "    QuestionTypes.COMPATIBILITY: {\n",
    "        \"avg_questions_per_item\": 1,\n",
    "        \"frac_thumbs_up\": 0.8,\n",
    "        \"recency_factor\": 1,\n",
    "    },\n",
    "    QuestionTypes.COUNTRY_OF_ORIGIN: {\n",
    "        \"avg_questions_per_item\": 1,\n",
    "        \"frac_thumbs_up\": 0.6,\n",
    "        \"recency_factor\": 0.7,\n",
    "    },\n",
    "    QuestionTypes.ENVIRONMENTAL: {\n",
    "        \"avg_questions_per_item\": 1,\n",
    "        \"frac_thumbs_up\": 0.1,\n",
    "        \"recency_factor\": 1,\n",
    "    },\n",
    "    QuestionTypes.AUTHENTIC: {\n",
    "        \"avg_questions_per_item\": 1,\n",
    "        \"frac_thumbs_up\": 0.1,\n",
    "        \"recency_factor\": 0.5,\n",
    "    },\n",
    "    QuestionTypes.MATERIALS: {\n",
    "        \"avg_questions_per_item\": 1,\n",
    "        \"frac_thumbs_up\": 0.2,\n",
    "        \"recency_factor\": 1,\n",
    "    },\n",
    "    QuestionTypes.TIME_SENSITIVE: {\n",
    "        \"avg_questions_per_item\": 3,\n",
    "        \"frac_thumbs_up\": 0.1,\n",
    "        \"recency_factor\": 1,\n",
    "    },\n",
    "    QuestionTypes.TREND: {\n",
    "        \"avg_questions_per_item\": 1,\n",
    "        \"frac_thumbs_up\": 0.1,\n",
    "        \"recency_factor\": 1,\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Product Data\n",
    "We embed product details in the prompt to generate questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lancedb\n",
    "from question_types import Product\n",
    "\n",
    "try:\n",
    "    db = lancedb.connect(\"../../week1_bootstrap_evals/lancedb\")\n",
    "    products = db.open_table(\"products\").to_pandas()[[\"title\", \"description\"]]\n",
    "    products = [\n",
    "        Product(title=row[\"title\"], description=row[\"description\"])\n",
    "        for _, row in products.iterrows()\n",
    "    ]\n",
    "except Exception as e:\n",
    "    print(f\"Error loading product data. Run the week1 course notebooks first to create the products DB\")\n",
    "    print(f\"Error: {str(e)}\")\n",
    "    products = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Generation Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dan/Desktop/systematically-improving-rag/systemic/lib/python3.10/site-packages/pydantic/_internal/_config.py:341: UserWarning: Valid config keys have changed in V2:\n",
      "* 'allow_population_by_field_name' has been renamed to 'populate_by_name'\n",
      "* 'smart_union' has been removed\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating evals: 1 validation error for Question\n",
      "  Invalid JSON: EOF while parsing a string at line 1 column 73 [type=json_invalid, input_value='{\"text\":\"Are the 12-poin... on a Toyota Prius?â€}', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.8/v/json_invalid\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from typing import List\n",
    "import instructor\n",
    "import json\n",
    "from openai import AsyncOpenAI\n",
    "from numpy.random import poisson, uniform\n",
    "from question_types import UntypedQuestion, Question, Product, question_type_details\n",
    "client = instructor.from_openai(AsyncOpenAI())\n",
    "\n",
    "async def generate_questions(\n",
    "    n_questions: int, product: Product, question_type: QuestionTypes, semaphore: asyncio.Semaphore, recency_factor: float = 1.0\n",
    ") -> List[UntypedQuestion]:\n",
    "    async with semaphore:\n",
    "        question_type_info = question_type_details[question_type]\n",
    "        question_type_title = question_type_info.title\n",
    "        question_type_description = question_type_info.description\n",
    "        question_type_examples = question_type_info.examples\n",
    "        recency_factor = question_type_stats[question_type][\"recency_factor\"]\n",
    "\n",
    "        prompt = f\"\"\"Create {n_questions} questions that someone might ask about a {product.title} before buying it online.\n",
    "\n",
    "The description of the {product.title} is: {product.description}\n",
    "\n",
    "Your questions should specifically be in the following category of questions: `{question_type_title}`.\n",
    "This category of questions questions is described as: `{question_type_description}`.\n",
    "\n",
    "Here are examples of questions in that category:\n",
    "`{question_type_examples[0]}`\n",
    "`{question_type_examples[1]}`\n",
    "\n",
    "The questions should be varied. Do not have duplicates.\n",
    "Use creative license to make the questions specific and concrete (e.g. you can make up other product names or product details to make concrete questions)\n",
    "Respond only with the list of questions.\"\"\"\n",
    "\n",
    "        frac_thumbs_up = question_type_stats[question_type][\"frac_thumbs_up\"]\n",
    "        try:\n",
    "            questions = client.chat.completions.create_iterable(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                response_model=Question,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            )\n",
    "            return [\n",
    "                UntypedQuestion(\n",
    "                    question=Question(text=q.text),\n",
    "                    product=product,\n",
    "                    thumbs_up=uniform() < frac_thumbs_up,\n",
    "                    days_ago=int(uniform(0, 30) * recency_factor)\n",
    "                )\n",
    "                async for q in questions\n",
    "            ]\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating evals: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "\n",
    "async def all_questions_for_qtype(question_type: QuestionTypes, semaphore: asyncio.Semaphore) -> List[UntypedQuestion]:\n",
    "    avg_questions_per_item = question_type_stats[question_type.value][\"avg_questions_per_item\"]\n",
    "    tasks = []\n",
    "    for product in products:\n",
    "        n_questions = poisson(avg_questions_per_item)\n",
    "        task = generate_questions(n_questions, product, question_type, semaphore)\n",
    "        tasks.append(task)\n",
    "    \n",
    "    results = await asyncio.gather(*tasks)\n",
    "    return [question for sublist in results for question in sublist]\n",
    "\n",
    "async def generate_all_questions():\n",
    "    semaphore = asyncio.Semaphore(50)\n",
    "    tasks = []\n",
    "    for qt in QuestionTypes:\n",
    "        task = all_questions_for_qtype(qt, semaphore)\n",
    "        tasks.append(task)\n",
    "    \n",
    "    results = await asyncio.gather(*tasks)\n",
    "    return [question for sublist in results for question in sublist]\n",
    "\n",
    "questions = await generate_all_questions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Questions\n",
    "\n",
    "We could also save it in LanceDB. But use JSON for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_question(q: UntypedQuestion):\n",
    "    return {\n",
    "        \"question\": q.question.text,\n",
    "        \"product\": q.product.dict(),\n",
    "        \"thumbs_up\": q.thumbs_up,\n",
    "        \"days_ago\": q.days_ago\n",
    "    }\n",
    "\n",
    "with open(\"prod_questions.json\", \"w\") as f:\n",
    "    serialized = [serialize_question(q) for q in questions]\n",
    "    json.dump(serialized, f, default=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "systemic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
