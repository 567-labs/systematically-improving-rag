{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Tool Call Choice\n",
    "\n",
    "You can build many tools. But calling all tools for all queries would be slow (and possibly harmful if they have side effects).\n",
    "\n",
    "We benchmark tool retrieval with synthetic questions much like how we measured document retrieval in week 1. This section shows how to create the tool retrieval benchmark, and we'll improve retrieval in week 4.\n",
    "\n",
    "\n",
    "## Load Products\n",
    "\n",
    "We will test tool recall on a sample of our product inventory. Weload these products below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Product(title='Electric Screwdriver', description='Featuring a built-in LED light, this electric screwdriver allows for visibility in dimly lit areas. It also has an automatic spindle lock for easy bit changes, enhancing productivity during tasks.'),\n",
       " Product(title='Belt Sander', description='With a sturdy build, this belt sander can tackle heavy-duty tasks while maintaining precision. Its quick-change belt system allows for rapid tool adjustments.'),\n",
       " Product(title='Sprayer', description='This versatile sprayer is ideal for painting, stain, and sealant applications. Its adjustable settings provide control over spray patterns and flow, letting you achieve the desired finish.')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "from typing import List, Union\n",
    "from pydantic import BaseModel\n",
    "import instructor\n",
    "from openai import AsyncOpenAI, OpenAI\n",
    "import lancedb\n",
    "import random\n",
    "\n",
    "\n",
    "class Product(BaseModel):\n",
    "    title: str\n",
    "    description: str\n",
    "\n",
    "\n",
    "try:\n",
    "    db = lancedb.connect(\"../week1_bootstrap_evals/lancedb\")\n",
    "    products = db.open_table(\"products\").to_pandas()[[\"title\", \"description\"]]\n",
    "    products = [\n",
    "        Product(title=row[\"title\"], description=row[\"description\"])\n",
    "        for _, row in products.iterrows()\n",
    "    ]\n",
    "except Exception as e:\n",
    "    print(\n",
    "        f\"Error loading product data. Run the week1 course notebooks first to create the products DB\"\n",
    "    )\n",
    "    print(f\"Error: {str(e)}\")\n",
    "\n",
    "random.sample(products, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Types for Tool Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShippingDateRequest(BaseModel):\n",
    "    call_name: str = \"ShippingDateRequest\"\n",
    "    sku: str\n",
    "\n",
    "\n",
    "class ShippingCostRequest(BaseModel):\n",
    "    call_name: str = \"ShippingCostRequest\"\n",
    "    sku: str\n",
    "    shipping_location: str\n",
    "\n",
    "\n",
    "class QuestionAboutImageRequest(BaseModel):\n",
    "    call_name: str = \"QuestionAboutImageRequest\"\n",
    "    image_url: str\n",
    "    question: str\n",
    "\n",
    "\n",
    "class ProductDimensionsRequest(BaseModel):\n",
    "    call_name: str = \"ProductDimensionsRequest\"\n",
    "    sku: str\n",
    "\n",
    "\n",
    "class PriceHistoryRequest(BaseModel):\n",
    "    call_name: str = \"PriceHistoryRequest\"\n",
    "    sku: str\n",
    "\n",
    "\n",
    "class ProductComparisonRequest(BaseModel):\n",
    "    call_name: str = \"ProductComparisonRequest\"\n",
    "    sku1: str\n",
    "    sku2: str\n",
    "\n",
    "\n",
    "class LogDesiredFeatureRequest(BaseModel):\n",
    "    call_name: str = \"LogDesiredFeatureRequest\"\n",
    "    sku: str\n",
    "    user_id: str\n",
    "    desired_feature: str\n",
    "\n",
    "\n",
    "class ExtractFromImageRequest(BaseModel):\n",
    "    call_name: str = \"ExtractFromImageRequest\"\n",
    "    image_url: str\n",
    "    question: str\n",
    "\n",
    "\n",
    "class ProductMaterialsRequest(BaseModel):\n",
    "    call_name: str = \"ProductMaterialsRequest\"\n",
    "    sku: str\n",
    "\n",
    "\n",
    "ToolType = Union[\n",
    "    ShippingDateRequest,\n",
    "    ShippingCostRequest,\n",
    "    QuestionAboutImageRequest,\n",
    "    ProductDimensionsRequest,\n",
    "    PriceHistoryRequest,\n",
    "    ProductComparisonRequest,\n",
    "    LogDesiredFeatureRequest,\n",
    "    ExtractFromImageRequest,\n",
    "    ProductMaterialsRequest,\n",
    "]\n",
    "\n",
    "all_tool_names = [cls.__name__ for cls in ToolType.__args__]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Synthetic Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 186 synthetic questions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[QuestionWithTools(question='Can it handle multiple tools at the same time without losing power?', required_tools=ToolNameList(tools=[]), product=Product(title='Air Compressor', description='With a powerful motor and quick recovery time, this air compressor delivers reliable performance for both home and professional use. It features multiple outlets for various tools.')),\n",
       " QuestionWithTools(question='I often work in poorly lit places. Can this help?', required_tools=ToolNameList(tools=[]), product=Product(title='Cordless Drill', description='Versatile and efficient, this cordless drill is designed for both drilling and driving screws with ease. It includes an LED light to illuminate dark work areas, ensuring you can work effectively at any time of day.')),\n",
       " QuestionWithTools(question='Is this hammer a good buy right now? Also, can you show more details in the image?', required_tools=ToolNameList(tools=['ExtractFromImageRequest', 'PriceHistoryRequest']), product=Product(title='Hammer', description='With a striking face designed for effective nail driving, this hammer is ideal for construction and home improvement projects. Its anti-shock handle reduces vibrations, enhancing user comfort.'))]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "async_client = instructor.from_openai(AsyncOpenAI())\n",
    "\n",
    "\n",
    "class ToolNameList(BaseModel):\n",
    "    tools: List[str]\n",
    "\n",
    "\n",
    "class QuestionWithTools(BaseModel):\n",
    "    question: str\n",
    "    required_tools: ToolNameList\n",
    "    product: Product\n",
    "\n",
    "\n",
    "def random_tool_selection() -> ToolNameList:\n",
    "    num_tools = random.choice([0, 1, 2])\n",
    "    selected_tools = random.sample(all_tool_names, num_tools)\n",
    "    return ToolNameList(tools=selected_tools)\n",
    "\n",
    "\n",
    "async def generate_synthetic_question(product: Product) -> QuestionWithTools:\n",
    "    tools_to_use = random_tool_selection()\n",
    "    prompt = f\"\"\"\n",
    "    Create a realistic question that a customer might ask an online chatbot about this product:\n",
    "    {product.title}: {product.description}\n",
    "\n",
    "    The customer knows this is just a programmatic chatbot. So they will be terse and lazy (possibly skipping whole/fully formed sentences).\n",
    "    \"\"\"\n",
    "    if tools_to_use:\n",
    "        prompt += f\"\"\"The question should require using these tools: {tools_to_use.tools}\n",
    "    \n",
    "    Do not explicitly ask for any tool/function.\n",
    "\n",
    "    For example:\n",
    "    Instead of asking `how long shipping will take`, say `I need it by Friday. Can you make it?`\n",
    "    Instead of asking for product dimensions, ask `Does this fit in a 3x7x4 case?`\n",
    "    Instead of asking for the price history, ask `Is now a good time to buy?`\n",
    "\n",
    "    Make it tricky to identify the tool(s) that would help an LLM to answer the question.\n",
    "    Real questions tend to be implicit.\n",
    "\n",
    "    Also, assume that we will not make a tool call to look something up if it is already in the product description.\n",
    "\n",
    "    Respond with the question.\n",
    "    \"\"\"\n",
    "    else:\n",
    "        prompt += f\"\"\"Respond with a question that can be answered without calling any of these tools:\n",
    "        {all_tool_names}\n",
    "        \"\"\"\n",
    "\n",
    "    question = await async_client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are creating synthetic questions for benchmarking tool retrieval in a retail chatbot.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        response_model=str,\n",
    "        temperature=1.0,\n",
    "    )\n",
    "    return QuestionWithTools(\n",
    "        question=question, required_tools=tools_to_use, product=product\n",
    "    )\n",
    "\n",
    "\n",
    "async def create_synthetic_dataset(\n",
    "    products: List[Product], questions_per_product: int = 1\n",
    ") -> List[QuestionWithTools]:\n",
    "    tasks = [\n",
    "        generate_synthetic_question(product)\n",
    "        for product in products\n",
    "        for _ in range(questions_per_product)\n",
    "    ]\n",
    "    return await asyncio.gather(*tasks)\n",
    "\n",
    "\n",
    "synthetic_questions = await create_synthetic_dataset(products, questions_per_product=2)\n",
    "print(f\"Generated {len(synthetic_questions)} synthetic questions\")\n",
    "\n",
    "random.sample(synthetic_questions, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing What We Call\n",
    "\n",
    "We'll have a function that's used to retrieve tools (so you can use it broadly), and then another function for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is this drill heavy for long use?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ToolNameList(tools=['ProductDimensionsRequest'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "async def choose_tools(question: str, product: Product) -> ToolNameList:\n",
    "    try:\n",
    "        response = await async_client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": f\"\"\"What tools will help you handle this user question? \n",
    "                    Respond with the names of 0, 1 or 2 tools from this list: \n",
    "                    {all_tool_names}.\n",
    "\n",
    "                    For context, this is a description of the product being referred to:\n",
    "                    {product.title}: {product.description}\n",
    "\n",
    "                    Don't make function calls to look up information that is already in the product description.\n",
    "                    \"\"\",\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": question},\n",
    "            ],\n",
    "            temperature=0.0,\n",
    "            response_model=ToolNameList,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error in API call: {str(e)}\")\n",
    "    return response\n",
    "\n",
    "example_question = synthetic_questions[0]\n",
    "print(example_question.question)\n",
    "await choose_tools(example_question.question, example_question.product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToolCallEvaluation(BaseModel):\n",
    "    question: str\n",
    "    expected_tools: ToolNameList\n",
    "    predicted_tools: ToolNameList\n",
    "\n",
    "\n",
    "async def get_tool_call_evals(q: QuestionWithTools) -> ToolCallEvaluation:\n",
    "    predicted_tools = await choose_tools(q.question, q.product)\n",
    "    return ToolCallEvaluation(\n",
    "        question=q.question,\n",
    "        expected_tools=q.required_tools,\n",
    "        predicted_tools=predicted_tools,\n",
    "    )\n",
    "\n",
    "\n",
    "async def run_evaluation(\n",
    "    synthetic_questions: List[QuestionWithTools], max_concurrency: int = 40\n",
    ") -> List[ToolCallEvaluation]:\n",
    "    semaphore = asyncio.Semaphore(max_concurrency)\n",
    "\n",
    "    async def bounded_get_tool_call_evals(q: QuestionWithTools):\n",
    "        async with semaphore:\n",
    "            return await get_tool_call_evals(q)\n",
    "\n",
    "    tasks = [bounded_get_tool_call_evals(q) for q in synthetic_questions]\n",
    "    return await asyncio.gather(*tasks)\n",
    "\n",
    "\n",
    "evaluation_results = await run_evaluation(synthetic_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.63\n",
      "Recall: 0.80\n"
     ]
    }
   ],
   "source": [
    "def calculate_precision_recall(evaluation_results: List[ToolCallEvaluation]):\n",
    "    true_positives = sum(\n",
    "        len(\n",
    "            set(result.expected_tools.tools).intersection(\n",
    "                set(result.predicted_tools.tools)\n",
    "            )\n",
    "        )\n",
    "        for result in evaluation_results\n",
    "    )\n",
    "    false_positives = sum(\n",
    "        len(set(result.predicted_tools.tools) - set(result.expected_tools.tools))\n",
    "        for result in evaluation_results\n",
    "    )\n",
    "    false_negatives = sum(\n",
    "        len(set(result.expected_tools.tools) - set(result.predicted_tools.tools))\n",
    "        for result in evaluation_results\n",
    "    )\n",
    "\n",
    "    precision = (\n",
    "        true_positives / (true_positives + false_positives)\n",
    "        if (true_positives + false_positives) > 0\n",
    "        else 0\n",
    "    )\n",
    "    recall = (\n",
    "        true_positives / (true_positives + false_negatives)\n",
    "        if (true_positives + false_negatives) > 0\n",
    "        else 0\n",
    "    )\n",
    "\n",
    "    return precision, recall\n",
    "\n",
    "\n",
    "precision, recall = calculate_precision_recall(evaluation_results)\n",
    "\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cadquery",
   "language": "python",
   "name": "cadquery"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
