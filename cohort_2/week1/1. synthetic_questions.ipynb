{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start with your Retrieval Pipeline\n",
    "\n",
    "A common problem is that end-to-end testing with language models is slow, expensive and hard to scale. It's extremely common to focus on evaluating the quality of generated content but this tends to be hard to scale and expensive to implement. \n",
    "\n",
    "This is where retrieval-focused evaluation shines. By focusing specifically on retrieval, we can test faster, iterate quicker and measure objectively. This is because retrieval metrics such as precision and recall are significantly faster and cheaper to compute. Most importantly, with a solid retrieval pipeline, we can start supporting several initial use cases to start gathering feedback on.\n",
    "\n",
    " \n",
    "| Aspect | Generation evals | Retrieval evals |\n",
    "|--------|------------------|-----------------|\n",
    "| Testing for | Team lead: \"Looks better now\" (based on metric evaluations that don't capture usage behavior) | Recall, precision@K |\n",
    "| Speed of tests | Slow (~1s to 10s per test) | Fast (~10ms to 800ms per test) |\n",
    "| Cost of tests | $100s per run | Negligible per run |\n",
    "| Frequency of tests | Infrequent | Frequent |\n",
    "| Speed of test iterations | Slow iterations (tests that could take minutes to hours) | Fast iterations |\n",
    "| Ease of test scalability | Difficult | Easy |\n",
    "\n",
    "## Synthetic Questions\n",
    "\n",
    "Since we may not have access to real user queries initially, generating synthetic questions allows us to simulate potential user interactions with our system. This approach helps us establish a performance baseline for our retrieval pipeline and identify any retrieval issues early on, ensuring that our system is robust before deployment. \n",
    "\n",
    "This is a never-ending process. Once you do get user traffic, you'll want to keep generating more synthetic questions and start blending in user queries to make sure your system's retrieval is still doing well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Improving Text-2-SQL with Synthetic Questions\n",
    "\n",
    "A common use case for Language Models is to answer questions with SQL using Text-2-SQL models. These take in a user query and output a SQL query which can be used to retrieve the relevant information. \n",
    "\n",
    "```\n",
    "Text : Hey could you help me find the top 5 most popular items in our store?\n",
    "Query: SELECT item_name, COUNT(*) as popularity FROM items GROUP BY item_name ORDER BY popularity DESC LIMIT 5\n",
    "```\n",
    "\n",
    "It's common to approach this as a generation task where the solution is to keep iterating on the prompt until the model outputs the correct SQL query. Generation is difficult to evaluate since we need to manually check the output. Automating this is a challenging task since a valid SQL query can be written in many different ways.\n",
    "\n",
    "Instead, it's much easier in this case to start by looking at the retrieval step. Since most companies have complex business use cases or might be using proprietary data, it's difficult to know what the final query should look like. That's where retrieval comes in.\n",
    "\n",
    "We can think of this as a retrieval task where we're trying to find the most relevant SQL query for a given user query. To do so, we can take each individual SQL query and generate a synthetic question for it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study : Bird-Bench\n",
    "\n",
    "For this case-study, we'll be using the Bird-Bench dataset. This is a huge Text-2-SQL dataset which originally contained a collection of text questions to a corresponding sql query. We'll be using the dev split of this dataset for this case study that provides ~1500+ sql snippets that involves ~95 different tables that we can use. \n",
    "\n",
    "Each example in our dataset contains three things\n",
    "\n",
    "- `id` : This is a unique identifier for each query\n",
    "- `query` : This is a sample SQL query \n",
    "- `difficulty` : This is a label that indicates how difficult the query is to generate. It can be either `simple`, `moderate` or `challenging`. For this case study, we'll only be using the `challenging` queries so that we can generate more difficult questions.\n",
    "\n",
    "Let's download our dataset and take a look at the first example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'query': \"SELECT `Free Meal Count (K-12)` / `Enrollment (K-12)` FROM frpm WHERE `County Name` = 'Alameda' ORDER BY (CAST(`Free Meal Count (K-12)` AS REAL) / `Enrollment (K-12)`) DESC LIMIT 1\",\n",
       " 'difficulty': 'simple'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "dataset = datasets.load_dataset(\"567-labs/bird-rag\")[\"train\"]\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we generate questions, let's take a look at what a synthetic question might look like by looking at our query below.\n",
    "\n",
    "> SELECT `Free Meal Count (K-12)` / `Enrollment (K-12)` FROM frpm WHERE `County Name` = 'Alameda' ORDER BY (CAST(`Free Meal Count (K-12)` AS REAL) / `Enrollment (K-12)`) DESC LIMIT 1\n",
    "\n",
    "\n",
    "We can see that this query is doing a few things\n",
    "\n",
    "1. First it's looking at the percentage of students that are recieving free meals \n",
    "2. It's restricting this to only schools in Alameda County\n",
    "3. Finally, it's ordering the results by this percentage in descending order and limiting the results to the top 1\n",
    "\n",
    "In short, it's trying to find the school in Alameda County with the highest percentage of students recieving free meals. So, what would be a potential question that we could ask which this query snippet would be highly relevant for?\n",
    "\n",
    "Well, a potential question could be\n",
    "\n",
    "- What is the school with the highest percentage of students recieving free meals in Alameda County?\n",
    "- How do we find the number of students recieving free meals in Seattle?\n",
    "\n",
    "These are queries which this snippet would be highly relevant for. Our goal with synthetic questions are to create a diverse set of questions here which would be able to test how consistently we retrieve the relevant snippet for a user query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Synthetic Questions\n",
    "\n",
    "Now that we understand what we're trying to do, let's see how we can load in our data and start generating synthetic questions. We'll do so in 3 steps\n",
    "\n",
    "1. Define our Pydantic models for our data\n",
    "2. Load in our data \n",
    "3. Start generating questions\n",
    "\n",
    "It's useful to start by defining models that explicitly represent the data that we want to work with. While this might seem like overkill for a small dataset, it ensures that our data is in the right format as we start generating our synthetic questions. \n",
    "\n",
    "For this specific example, we'll have 3 different pydantic models\n",
    "\n",
    "1. `Chunk` : This represents how we're representing our data from the dataset\n",
    "2. `Question` : This is the synthetic question that we'll be getting our model to generate\n",
    "3. `ChunkEval` : This represents a single question-chunk pair that we'll be uploading to Braintrust as a dataset later on to be used for benchmarking in `benchmark_retrieval.py`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Chunk(BaseModel):\n",
    "    chunk_id: str\n",
    "    text: str\n",
    "\n",
    "\n",
    "class Question(BaseModel):\n",
    "    chain_of_thought: str\n",
    "    question: str\n",
    "\n",
    "\n",
    "class ChunkEval(BaseModel):\n",
    "    chunk_id: str\n",
    "    question: str\n",
    "    chunk: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Structured Outputs to Generate Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how we can generate a question for a given SQL snippet. \n",
    "\n",
    "Remember that we want to generate a question that should either be answerable by the data returned by the SQL snippet or a snippet that is similar to the SQL snippet. To do so, we'll be using `instructor` to handle our generation. This is mainly due to two reasons\n",
    "\n",
    "1. We get to pass in a Pydantic object as a response model which helps with error handling and ensures that the output is in the right format\n",
    "2. With `jinja` templating, we can easily format our prompt by using `jinja` variables and using the `context` object to pass in the relevant data.\n",
    "\n",
    "You can definitely use the default OpenAI client but it's much simpler to just use `instructor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the highest proportion of free meals provided to the enrolled K-12 students in any school district within Alameda County?\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import instructor\n",
    "from asyncio import Semaphore, timeout\n",
    "from tqdm.asyncio import tqdm_asyncio as asyncio\n",
    "from tenacity import retry, stop_after_attempt, wait_fixed\n",
    "\n",
    "client = instructor.from_openai(openai.OpenAI())\n",
    "\n",
    "sql_snippet = \"SELECT `Free Meal Count (K-12)` / `Enrollment (K-12)` FROM frpm WHERE `County Name` = 'Alameda' ORDER BY (CAST(`Free Meal Count (K-12)` AS REAL) / `Enrollment (K-12)`) DESC LIMIT 1\"\n",
    "\n",
    "resp = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\"\"\n",
    "        Generate a question and answer pair that the following SQL snippet below will be able to answer. \n",
    "\n",
    "        The question should \n",
    "        - Be answerable only by the data that will be returned by the SQL snippet\n",
    "        - Not mention specific information in the SQL snippet directly\n",
    "        \n",
    "        \n",
    "        SQL Snippet:\n",
    "        {{ snippet }}\n",
    "        \"\"\",\n",
    "        }\n",
    "    ],\n",
    "    response_model=Question,\n",
    "    context={\"snippet\": sql_snippet},\n",
    ")\n",
    "\n",
    "print(resp.question)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated question is great, it's not only answerable by the data returned by the SQL snippet but it also doesn't mention specific information in the SQL snippet directly. Instead, it references the end result of why we would want the data - to make some conclusion about why we would want to find the school with the highest percentage of students recieving free meals in Alameda County.\n",
    "\n",
    "When generating a large amount of data at a single go, it's useful here to keep two things in mind\n",
    "\n",
    "1. We want to stay within our rate limits\n",
    "2. We need to make sure we're generating data in the form that we want\n",
    "\n",
    "It's useful therefore to always generate a few small samples, ensure that the output is in the right format and then scale up from there. Our questions seem to be in the right format so let's scale up. To do so, we'll use the `asyncio` library to run our requests in parallel, a semaphore to limit the number of concurrent requests and the async OpenAI client to handle our requests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 145/145 [00:46<00:00,  3.13it/s]\n"
     ]
    }
   ],
   "source": [
    "client = instructor.from_openai(openai.AsyncOpenAI())\n",
    "\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_fixed(10))\n",
    "async def generate_questions(chunk: Chunk, sem: Semaphore) -> ChunkEval:\n",
    "    async with sem, timeout(30):\n",
    "        resp = await client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"\"\"\n",
    "                Generate a question based off the following SQL snippet. Make sure that it's a question that can only be answered by the SQL snippet.\n",
    "\n",
    "                \n",
    "                Make the question more challenging by doing one of the following.\n",
    "                - Modifying the time period that we're filtering on (Eg. if the snippet has filtered by the entire year, look at 6 months or 1.5 years)\n",
    "                - Changing the value of the filter (Eg. if the snippet is looking at the results in Canada, change the question to ask about Toyko or Mexico instead)\n",
    "                - Adding in some useless information: Eg. Add information about the weather, a random event or a backstory that isn't mentioned in the snippet.\n",
    "\n",
    "                The final question should be at most 2 sentences long.\n",
    "\n",
    "                SQL Snippet:\n",
    "                {{ snippet }}\n",
    "                \"\"\",\n",
    "                }\n",
    "            ],\n",
    "            response_model=Question,\n",
    "            context={\"snippet\": chunk.text},\n",
    "            timeout=20,\n",
    "        )\n",
    "\n",
    "        return ChunkEval(\n",
    "            chunk_id=chunk.chunk_id,\n",
    "            question=resp.question,\n",
    "            chunk=chunk.text,\n",
    "        )\n",
    "\n",
    "\n",
    "sem = Semaphore(10)\n",
    "dataset = [\n",
    "    item\n",
    "    for item in datasets.load_dataset(\"567-labs/bird-rag\")[\"train\"]\n",
    "    if item[\"difficulty\"] == \"challenging\"\n",
    "]\n",
    "dataset = [Chunk(chunk_id=item[\"id\"], text=item[\"query\"]) for item in dataset]\n",
    "\n",
    "coros = [generate_questions(chunk, sem) for chunk in dataset]\n",
    "questions: list[ChunkEval] = await asyncio.gather(*coros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've generated our questions, let's take a look at what they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">During a scientific conference in Tokyo, researchers are exploring new compounds, specifically those related to \n",
       "molecule_id <span style=\"color: #008000; text-decoration-color: #008000\">'TR120'</span>. What distinct elements can be identified from this molecule?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "During a scientific conference in Tokyo, researchers are exploring new compounds, specifically those related to \n",
       "molecule_id \u001b[32m'TR120'\u001b[0m. What distinct elements can be identified from this molecule?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">SELECT DISTINCT T1.element, T2.label FROM atom AS T1 INNER JOIN molecule AS T2 ON T1.molecule_id = T2.molecule_id \n",
       "WHERE T2.molecule_id = <span style=\"color: #008000; text-decoration-color: #008000\">'TR060'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "SELECT DISTINCT T1.element, T2.label FROM atom AS T1 INNER JOIN molecule AS T2 ON T1.molecule_id = T2.molecule_id \n",
       "WHERE T2.molecule_id = \u001b[32m'TR060'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich import print\n",
    "\n",
    "questions[20]\n",
    "print(questions[20].question)\n",
    "print(questions[20].chunk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for each individual snippet, we generated a question that directly requires the SQL snippet's for data or for reference to be answered. This is crucial in ensuring that we're able to evaluate our retrieval pipeline well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading our Dataset\n",
    "\n",
    "> Braintrust is a end to end platform for building LLM applications. We'll be using it in `benchmark_retrieval.ipynb` to also evaluate our questions\n",
    "\n",
    "When running eval datasets, you need to start thinking about where to store them. We recommend using braintrust for this, it simplifies a significant amount of the process and makes it easier to keep track of all your experiments/production logs.\n",
    "\n",
    "Uploading a dataset is as simple as calling `braintrust.init_dataset` with the project and name of the dataset that we want to create. Note here that we're passing in the `chunk_id` as the `id` for each question. This helps us to avoid uploading duplicate questions when creating our dataset.\n",
    "\n",
    "You can verify that this is the case by running the cell a few times and seeing that the total number of questions stays the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DatasetSummary</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">project_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Text-2-SQL'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">dataset_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Bird-Bench-Questions'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">project_url</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'https://www.braintrust.dev/app/567/p/Text-2-SQL'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">dataset_url</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'https://www.braintrust.dev/app/567/p/Text-2-SQL/datasets/Bird-Bench-Questions'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">data_summary</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DataSummary</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">new_records</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">145</span>, <span style=\"color: #808000; text-decoration-color: #808000\">total_records</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">145</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDatasetSummary\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mproject_name\u001b[0m=\u001b[32m'Text-2-SQL'\u001b[0m,\n",
       "    \u001b[33mdataset_name\u001b[0m=\u001b[32m'Bird-Bench-Questions'\u001b[0m,\n",
       "    \u001b[33mproject_url\u001b[0m=\u001b[32m'https://www.braintrust.dev/app/567/p/Text-2-SQL'\u001b[0m,\n",
       "    \u001b[33mdataset_url\u001b[0m=\u001b[32m'https://www.braintrust.dev/app/567/p/Text-2-SQL/datasets/Bird-Bench-Questions'\u001b[0m,\n",
       "    \u001b[33mdata_summary\u001b[0m=\u001b[1;35mDataSummary\u001b[0m\u001b[1m(\u001b[0m\u001b[33mnew_records\u001b[0m=\u001b[1;36m145\u001b[0m, \u001b[33mtotal_records\u001b[0m=\u001b[1;36m145\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import braintrust\n",
    "\n",
    "\n",
    "dataset = braintrust.init_dataset(project=\"Text-2-SQL\", name=\"Bird-Bench-Questions\")\n",
    "for question in questions:\n",
    "    dataset.insert(\n",
    "        id=question.chunk_id,\n",
    "        input=question.question,\n",
    "        expected=[question.chunk],\n",
    "        metadata={\"chunk_id\": question.chunk_id, \"chunk\": question.chunk},\n",
    "    )\n",
    "\n",
    "print(dataset.summarize())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
