{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking Retrieval Methods\n",
    "\n",
    "> If you have not already, please run the `1. synthetic_questions.ipynb` notebook to generate the synthetic questions and chunks before proceeding.\n",
    "\n",
    "Remember that in our previous notebook, we had a dataset that had the following three fields:\n",
    "\n",
    "- `id` : This is a unique identifier for each query\n",
    "- `query` : This is a sample SQL query \n",
    "- `difficulty` : This is a label that indicates how difficult the query is to generate. It can be either `simple`, `moderate` or `challenging`. \n",
    "\n",
    "We then generated a dataset of synthetic questions for each query that was marked as `challenging` in the dataset before storing it in braintrust. \n",
    "\n",
    "With these new questions, we can now benchmark the performance of embedding search vs hybrid search. Ultimately, we'll use braintrust to collect all the data and discuss the tradeoffs of each method compared to performance and complexity.\n",
    "\n",
    "## Evaluation Method\n",
    "\n",
    "We'll do so in two steps\n",
    "\n",
    "1. First, we'll ingest all of the queries in our dataset into a single lancedb database table. \n",
    "2. Next, we'll write a function that will take in a query and return the top k results from our database.\n",
    "\n",
    "We can then measure recall@k and MRR@k by verifying that using the `question` we can retrieve the `chunk_id` correctly from our datbase when we look at the top k most relevant results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Our Rag Pipeline\n",
    "\n",
    "In this example, we're using a local `lancedb` instance. We're doing so because of 3 reasons.\n",
    "\n",
    "1. LanceDB handles the embeddings of our data for us \n",
    "2. It provides embedding search, hybrid search and other re-ranking methods all within a single api.\n",
    "3. We can use Pydantic to handle the creation and ingestion of our data.\n",
    "\n",
    "This makes it quick and easy for us to compare the performance of each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1528 chunks ingested into the database\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import lancedb\n",
    "from lancedb.pydantic import LanceModel, Vector\n",
    "from lancedb.embeddings import get_registry\n",
    "\n",
    "# Create LanceDB Instance\n",
    "db = lancedb.connect(\"./lancedb\")\n",
    "\n",
    "# Define and create Table using Pydantic\n",
    "func = get_registry().get(\"openai\").create(name=\"text-embedding-3-small\")\n",
    "\n",
    "\n",
    "class Chunk(LanceModel):\n",
    "    id: str\n",
    "    query: str = func.SourceField()\n",
    "    vector: Vector(func.ndims()) = func.VectorField()\n",
    "\n",
    "\n",
    "table = db.create_table(\"chunks\", schema=Chunk, mode=\"overwrite\")\n",
    "\n",
    "# Ingest dataset into table\n",
    "dataset = datasets.load_dataset(\"567-labs/bird-rag\")[\"train\"]\n",
    "formatted_dataset = [{\"id\": item[\"id\"], \"query\": item[\"query\"]} for item in dataset]\n",
    "\n",
    "table.add(formatted_dataset)\n",
    "print(f\"{table.count_rows()} chunks ingested into the database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Metrics\n",
    "\n",
    "Let's now start by evaluating the retrieval performance of our model. We'll do so by measuring the recall and MRR at different levels of k.\n",
    "\n",
    "<TODO: Redefine metrics here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mrr(predictions: list[str], gt: list[str]):\n",
    "    mrr = 0\n",
    "    for label in gt:\n",
    "        if label in predictions:\n",
    "            mrr = max(mrr, 1 / (predictions.index(label) + 1))\n",
    "    return mrr\n",
    "\n",
    "\n",
    "def calculate_recall(predictions: list[str], gt: list[str]):\n",
    "    return len([label for label in gt if label in predictions]) / len(gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Metrics\n",
    "\n",
    "Now let's see how we can compute these using braintrust and lancedb. Let's start by writing a function that will take a query and return the top k results from our database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"SELECT T1.driverId FROM lapTimes AS T1 INNER JOIN races AS T2 on T1.raceId = T2.raceId WHERE T2.name = 'French Grand Prix' AND T1.lap = 3 ORDER BY T1.time DESC LIMIT 1\",\n",
       " 'SELECT T2.City FROM frpm AS T1 INNER JOIN schools AS T2 ON T1.CDSCode = T2.CDSCode GROUP BY T2.City ORDER BY SUM(T1.`Enrollment (K-12)`) ASC LIMIT 5']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Make this more complex to support a larger max_k, re-rankers etc + mode\n",
    "def retrieve(query: str, k: int = 10) -> list[str]:\n",
    "    results = table.search(query).limit(k).to_list()\n",
    "    return [result[\"query\"] for result in results]\n",
    "\n",
    "\n",
    "retrieve(\"What is the capital of France?\")[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define a simple list of functions that will take in these returned results and compute the recall and MRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "eval_metrics = [[\"mrr\", calculate_mrr], [\"recall\", calculate_recall]]\n",
    "sizes = [1,3, 5, 10]\n",
    "\n",
    "metrics = {\n",
    "    f\"{metric_name}@{size}\": lambda predictions, gt, m=metric_fn, s=size: (\n",
    "        lambda p, g: m(p[:s], g)\n",
    "    )(predictions, gt)\n",
    "    for (metric_name, metric_fn), size in itertools.product(eval_metrics, sizes)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment add-braintrust-support-1729604364 is running at https://www.braintrust.dev/app/567/p/Text-2-SQL/experiments/add-braintrust-support-1729604364\n",
      "Text-2-SQL (data): 290it [00:01, 169.57it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53940630558641f4b481f71ba16983af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text-2-SQL (tasks):   0%|          | 0/290 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================SUMMARY=========================\n",
      "add-braintrust-support-1729604364 compared to add-braintrust-support-1729604072:\n",
      "58.39% 'mrr@3'     score\n",
      "59.79% 'mrr@5'     score\n",
      "61.35% 'mrr@10'    score\n",
      "61.70% 'mrr@15'    score\n",
      "61.85% 'mrr@25'    score\n",
      "73.45% 'recall@3'  score\n",
      "79.31% 'recall@5'  score\n",
      "90.69% 'recall@10' score\n",
      "95.17% 'recall@15' score\n",
      "97.93% 'recall@25' score\n",
      "\n",
      "7.12s duration\n",
      "\n",
      "See results for add-braintrust-support-1729604364 at https://www.braintrust.dev/app/567/p/Text-2-SQL/experiments/add-braintrust-support-1729604364\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EvalResultWithSummary(summary=\"...\", results=[...])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from braintrust import Score, init_dataset, Eval\n",
    "\n",
    "\n",
    "def evaluate_braintrust(input, output, **kwargs):\n",
    "    hashed_queries = [hash_query(query) for query in output]\n",
    "    hashed_expected = [hash_query(query) for query in kwargs[\"expected\"]]\n",
    "    return [\n",
    "        Score(\n",
    "            name=metric,\n",
    "            score=score_fn(hashed_queries, hashed_expected),\n",
    "            metadata={\"query\": input, \"result\": output, **kwargs[\"metadata\"]},\n",
    "        )\n",
    "        for metric, score_fn in metrics.items()\n",
    "    ]\n",
    "\n",
    "\n",
    "await Eval(\n",
    "    \"Text-2-SQL\",\n",
    "    data=init_dataset(project=\"Text-2-SQL\", name=\"Bird-Bench-Questions\"),\n",
    "    task=lambda input: retrieve(input, 25),\n",
    "    scores=[evaluate_braintrust],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Add in more complex metrics here for different stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
