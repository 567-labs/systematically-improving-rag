{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook, we'll use a simple pipeline to generate synthetic questions. We'll start with a open source hugging face dataset of SQL queries and then generate questions from them. Once we've done so, we can use a RAG system to answer these questions and then evaluate the recall of the RAG system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting Our Data\n",
    "\n",
    "Let's start by defining the data types we'll use throughout the notebook.\n",
    "\n",
    "1. Chunk: This represents the data that we'll be using to generate questions with\n",
    "2. QuestionAnswer: This represents a question and answer pair generated by a language model\n",
    "3. ChunkEval: This represents a single evaluation that we'll be using to recall the RAG system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class Chunk(BaseModel):\n",
    "    chunk_id: str\n",
    "    text:str\n",
    "    metadata:dict\n",
    "\n",
    "class QuestionAnswer(BaseModel):\n",
    "    chain_of_thought:str\n",
    "    question:str\n",
    "    answer:str\n",
    "\n",
    "class ChunkEval(BaseModel):\n",
    "    chunk_id:str\n",
    "    question:str\n",
    "    answer:str\n",
    "    chunk:str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this specific example, we'll be using the Bird-Bench dataset. We've uploaded it ahead of time to Hugging Face so let's load it in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Consider the average difference between K-12 enrollment and 15-17 enrollment of schools that are locally funded, list the names and DOC type of schools which has a difference above this average.',\n",
       " 'labels': [\"SELECT T2.School, T2.DOC FROM frpm AS T1 INNER JOIN schools AS T2 ON T1.CDSCode = T2.CDSCode WHERE T2.FundingType = 'Locally funded' AND (T1.`Enrollment (K-12)` - T1.`Enrollment (Ages 5-17)`) > (SELECT AVG(T3.`Enrollment (K-12)` - T3.`Enrollment (Ages 5-17)`) FROM frpm AS T3 INNER JOIN schools AS T4 ON T3.CDSCode = T4.CDSCode WHERE T4.FundingType = 'Locally funded')\"],\n",
       " 'query': \"SELECT T2.School, T2.DOC FROM frpm AS T1 INNER JOIN schools AS T2 ON T1.CDSCode = T2.CDSCode WHERE T2.FundingType = 'Locally funded' AND (T1.`Enrollment (K-12)` - T1.`Enrollment (Ages 5-17)`) > (SELECT AVG(T3.`Enrollment (K-12)` - T3.`Enrollment (Ages 5-17)`) FROM frpm AS T3 INNER JOIN schools AS T4 ON T3.CDSCode = T4.CDSCode WHERE T4.FundingType = 'Locally funded')\",\n",
       " 'metadata': {'db_id': 'california_schools',\n",
       "  'difficulty': 'challenging',\n",
       "  'evidence': 'Difference between K-12 enrollment and 15-17 enrollment can be computed by `Enrollment (K-12)` - `Enrollment (Ages 5-17)`',\n",
       "  'query': \"SELECT T2.School, T2.DOC FROM frpm AS T1 INNER JOIN schools AS T2 ON T1.CDSCode = T2.CDSCode WHERE T2.FundingType = 'Locally funded' AND (T1.`Enrollment (K-12)` - T1.`Enrollment (Ages 5-17)`) > (SELECT AVG(T3.`Enrollment (K-12)` - T3.`Enrollment (Ages 5-17)`) FROM frpm AS T3 INNER JOIN schools AS T4 ON T3.CDSCode = T4.CDSCode WHERE T4.FundingType = 'Locally funded')\",\n",
       "  'question_id': 28}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "dataset = datasets.load_dataset(\"567-labs/bird-dev-snippets\")\n",
    "\n",
    "# We only take challenging questions\n",
    "challenging_questions = [item for item in dataset[\"original\"] if item['metadata']['difficulty'] == 'challenging']\n",
    "challenging_questions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our dataset, let's format it into our Chunk data type. Once we've done so, we can start generating synthetic questions that we can use to evaluate our RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"chunk_id\": \"0c19c282e65f21e5acb0809c95b3fffcb077b434b6ee137390068454a01d8b6a\",\n",
      "  \"text\": \"SELECT T2.School, T2.DOC FROM frpm AS T1 INNER JOIN schools AS T2 ON T1.CDSCode = T2.CDSCode WHERE T2.FundingType = 'Locally funded' AND (T1.`Enrollment (K-12)` - T1.`Enrollment (Ages 5-17)`) > (SELECT AVG(T3.`Enrollment (K-12)` - T3.`Enrollment (Ages 5-17)`) FROM frpm AS T3 INNER JOIN schools AS T4 ON T3.CDSCode = T4.CDSCode WHERE T4.FundingType = 'Locally funded')\",\n",
      "  \"metadata\": {\n",
      "    \"db_id\": \"california_schools\",\n",
      "    \"difficulty\": \"challenging\",\n",
      "    \"evidence\": \"Difference between K-12 enrollment and 15-17 enrollment can be computed by `Enrollment (K-12)` - `Enrollment (Ages 5-17)`\",\n",
      "    \"query\": \"SELECT T2.School, T2.DOC FROM frpm AS T1 INNER JOIN schools AS T2 ON T1.CDSCode = T2.CDSCode WHERE T2.FundingType = 'Locally funded' AND (T1.`Enrollment (K-12)` - T1.`Enrollment (Ages 5-17)`) > (SELECT AVG(T3.`Enrollment (K-12)` - T3.`Enrollment (Ages 5-17)`) FROM frpm AS T3 INNER JOIN schools AS T4 ON T3.CDSCode = T4.CDSCode WHERE T4.FundingType = 'Locally funded')\",\n",
      "    \"question_id\": 28\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "\n",
    "def hash_query(query:str) -> str:\n",
    "    return hashlib.sha256(query.encode()).hexdigest()\n",
    "\n",
    "chunks = [\n",
    "    Chunk(chunk_id=hash_query(item['query']), text=item['query'], metadata=item['metadata'])\n",
    "    for item in challenging_questions\n",
    "]\n",
    "\n",
    "print(chunks[0].model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Questions\n",
    "\n",
    "Now that we've formatted our data, we can start generating some synthetic questions. We'll be using Open AI's gpt-4o model to generate these questions. Note here that we're using a relatively simple prompt to generate questions, you can definitely experiment with more complex prompts to generate better questions. But it's always best to start simple.\n",
    "\n",
    "Because we're going to be generating a large amount of questions at once, we'll be using the `instructor` library and running our requests in parallel to speed things up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import instructor\n",
    "from asyncio import Semaphore\n",
    "from tqdm.asyncio import tqdm_asyncio as asyncio\n",
    "\n",
    "client = instructor.from_openai(openai.AsyncOpenAI())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`instructor` supports complex prompt templating using the `jinja` templating language. This makes it easy for us to format our prompts by using `jinja` variables and using the `context` object to pass in the relevant data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 145/145 [01:49<00:00,  1.33it/s]\n"
     ]
    }
   ],
   "source": [
    "async def generate_questions(chunk:Chunk,sem:Semaphore) -> ChunkEval:\n",
    "    async with sem:\n",
    "        resp = await client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\n",
    "                \"role\":\"user\",\n",
    "                \"content\":\"\"\"\n",
    "                Generate a question and answer pair that the following SQL snippet below will be able to answer. \n",
    "\n",
    "                The question should \n",
    "                - Be answerable only by the data that will be returned by the SQL snippet\n",
    "                - Not mention specific information in the SQL snippet directly\n",
    "\n",
    "                SQL Snippet:\n",
    "                {{ snippet }}\n",
    "                \"\"\"\n",
    "            }],\n",
    "            response_model=QuestionAnswer,\n",
    "            context = {\n",
    "                \"snippet\":chunk.text\n",
    "            },\n",
    "            timeout=20\n",
    "        )\n",
    "\n",
    "        return ChunkEval(\n",
    "            chunk_id=chunk.chunk_id,\n",
    "            question=resp.question,\n",
    "            answer=resp.answer,\n",
    "            chunk=chunk.text\n",
    "        )\n",
    "\n",
    "sem = Semaphore(10)\n",
    "coros = [generate_questions(chunk,sem) for chunk in chunks]\n",
    "questions:list[ChunkEval] = await asyncio.gather(*coros)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"chunk_id\": \"0c19c282e65f21e5acb0809c95b3fffcb077b434b6ee137390068454a01d8b6a\",\n",
      "  \"question\": \"Which locally funded schools have a significantly higher proportion of enrollment outside the ages 5-17 compared to the average locally funded school?\",\n",
      "  \"answer\": \"The selected schools and their DOCs that have a greater difference between total K-12 enrollment and ages 5-17 enrollment than the average among locally funded schools.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(questions[0].model_dump_json(indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading our Dataset\n",
    "\n",
    "Now that we've generated our questions, let's upload them to Braintrust so that we can use it in our Evaluation later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================SUMMARY=========================\n",
      "Total records: 145 (145 new or updated records)\n",
      "See results for all datasets in Retrieval at https://www.braintrust.dev/app/567/p/Retrieval\n",
      "See results for Synthetic Questions at https://www.braintrust.dev/app/567/p/Retrieval/datasets/Synthetic%20Questions\n"
     ]
    }
   ],
   "source": [
    "import braintrust\n",
    "\n",
    "\n",
    "dataset = braintrust.init_dataset(project=\"Retrieval\", name=\"Synthetic Questions\")\n",
    "for question in questions:\n",
    "    dataset.insert(\n",
    "        input=question.question,\n",
    "        expected=[question.chunk],\n",
    "        metadata={\n",
    "\n",
    "            \"chunk_id\":question.chunk_id,\n",
    "            \"chunk\":question.chunk\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(dataset.summarize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
