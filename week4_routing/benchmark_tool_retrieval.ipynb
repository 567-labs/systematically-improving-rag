{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Tool Call Choice\n",
    "\n",
    "This week we focused on routers to determine which functions to call or which indices to use for each user-supplied question.\n",
    "\n",
    "This notebook shows how to benchmark function retrieval with synthetic data. It import utils.py which has some more reusable functions.\n",
    "\n",
    "This approach mirrors how we used synthetic questions to measured document retrieval in week 1. \n",
    "\n",
    "## Load Products\n",
    "\n",
    "We test tool-recall with a sample of our product inventory. We load these products below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Product(title='Ladder', description='This 8-foot fiberglass ladder is perfect for electrical work. The wide steps provide stability and comfort.'),\n",
       " Product(title='Air Compressor', description='A 6-gallon pancake air compressor with a high-efficiency motor. It is perfect for powering air tools and inflating tires.'),\n",
       " Product(title='Paint Roller', description='A 4-inch mini paint roller ideal for touch-ups and small projects. The ergonomic handle provides a comfortable grip.')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "import instructor\n",
    "from openai import AsyncOpenAI\n",
    "import lancedb\n",
    "import random\n",
    "\n",
    "from funcs_to_call import FunctionOption\n",
    "from utils import (\n",
    "    calculate_precision_recall,\n",
    "    FunctionList,\n",
    "    QuestionWithTools,\n",
    "    get_all_tool_call_evals,\n",
    "    describe_tools,\n",
    ")\n",
    "\n",
    "\n",
    "class Product(BaseModel):\n",
    "    title: str\n",
    "    description: str\n",
    "\n",
    "\n",
    "LANCEDB_PATH = \"../week1_bootstrap_evals/lancedb\"\n",
    "\n",
    "try:\n",
    "    db = lancedb.connect(LANCEDB_PATH)\n",
    "    products = db.open_table(\"products\").to_pandas()\n",
    "    products = [\n",
    "        Product(title=row[\"title\"], description=row[\"description\"])\n",
    "        for _, row in products.iterrows()\n",
    "    ]\n",
    "except Exception as e:\n",
    "    print(\n",
    "        f\"Error loading product data. Run the week1 course notebooks first to create the products DB\"\n",
    "    )\n",
    "    print(f\"Error: {str(e)}\")\n",
    "\n",
    "random.sample(products, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a string listing all available functions to call. We will use this in our prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShippingDateRequest: Check when a product will be shipped\n",
      "ShippingCostRequest: Check the cost of shipping a product\n",
      "ProductDimensionsRequest: Check the dimensions of a product\n",
      "PriceHistoryRequest: Check the price history of a product (e.g. identifying historical price fluctuations)\n",
      "ProductComparisonRequest: Compare two products\n",
      "LogDesiredFeatureRequest: Record a user's desire for a certain product feature\n",
      "ExtractDataFromImageRequest: Use our product images with multimodal llm to extract info about the product\n",
      "ProductMaterialsRequest: Check what materials a product is made of\n"
     ]
    }
   ],
   "source": [
    "tool_list = describe_tools(FunctionOption.__args__)\n",
    "print(tool_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Synthetic Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 180 synthetic questions. Here is a sample:\n",
      "Question: What attachments come with the 5-gallon shop vacuum?\n",
      "    \n",
      "    For context, here is the product description:\n",
      "    Product Description: Shop Vacuum: A 5-gallon wet/dry shop vacuum with powerful suction. The included attachments make it versatile for various cleaning tasks.\n",
      "    \n",
      "func_names=[]\n",
      "---\n",
      "Question: Can you tell me if this paint roller has always been this price? Also, what are the shipping costs and when can I expect it to arrive? By the way, is it made of durable materials? And how does it compare to other mini paint rollers?\n",
      "    \n",
      "    For context, here is the product description:\n",
      "    Product Description: Paint Roller: A 4-inch mini paint roller ideal for touch-ups and small projects. The ergonomic handle provides a comfortable grip.\n",
      "    \n",
      "func_names=['PriceHistoryRequest']\n",
      "---\n",
      "Question: Do these glasses have UV protection?\n",
      "    \n",
      "    For context, here is the product description:\n",
      "    Product Description: Safety Glasses: A pair of tinted safety glasses for outdoor use. The UV protection shields your eyes from harmful rays.\n",
      "    \n",
      "func_names=[]\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "async_client = instructor.from_openai(AsyncOpenAI())\n",
    "\n",
    "def add_context_to_question(question: str, product: Product) -> str:\n",
    "    return f\"\"\"Question: {question}\n",
    "    \n",
    "    For context, here is the product description:\n",
    "    Product Description: {product.title}: {product.description}\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def random_tool_selection() -> List[FunctionOption]:\n",
    "    num_tools = random.choice([0, 1, 2])\n",
    "    return random.sample(FunctionOption.__args__, num_tools)\n",
    "\n",
    "\n",
    "async def generate_synthetic_question(product: Product) -> QuestionWithTools:\n",
    "    tools_to_use = random_tool_selection()\n",
    "    prompt = f\"\"\"\n",
    "    Create a realistic question a customer might ask a support chatbot about this product:\n",
    "    {product.title}: {product.description}\n",
    "\n",
    "    The customer knows this is a programmatic chatbot. So they will be terse and lazy (possibly skipping whole/fully formed sentences).\n",
    "    \"\"\"\n",
    "    if tools_to_use:\n",
    "        prompt += f\"\"\"The question should require using these function calls: {tool_list}\n",
    "    \n",
    "    Do not explicitly ask for the function. Instead, ask a question that happens to answerable by calling the function.\n",
    "\n",
    "    For example:\n",
    "    Instead of asking `how long shipping will take`, say `I need it by Friday. Can you make it?`\n",
    "    Instead of asking for product dimensions, ask `Would this fit in a 3x7x4 case?`\n",
    "    Instead of asking for the price history, ask `Is now a good time to buy?`\n",
    "\n",
    "    Real questions tend to be implicit.\n",
    "    Ask questions where it is hard to identify what tool(s) would help an LLM to answer the question.\n",
    "    Assume that we will not make a tool call to look something up if it is already in the product description.\n",
    "\n",
    "    Respond with the question.\n",
    "    \"\"\"\n",
    "    else:\n",
    "        prompt += f\"\"\"Respond with a question that can be answered from the product description without calling any functions:\n",
    "        {tool_list}\n",
    "        \"\"\"\n",
    "\n",
    "    question = await async_client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are creating synthetic questions for benchmarking tool retrieval in a retail chatbot.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        response_model=str,\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    tools_names = FunctionList(func_names=[tool.__name__ for tool in tools_to_use])\n",
    "    question_with_context = add_context_to_question(question, product)\n",
    "    return QuestionWithTools(question=question_with_context, required_tools=tools_names)\n",
    "\n",
    "\n",
    "async def create_synthetic_dataset(\n",
    "    products: List[Product], questions_per_product: int\n",
    ") -> List[QuestionWithTools]:\n",
    "    tasks = [\n",
    "        generate_synthetic_question(product)\n",
    "        for product in products\n",
    "        for _ in range(questions_per_product)\n",
    "    ]\n",
    "    return await asyncio.gather(*tasks)\n",
    "\n",
    "\n",
    "synthetic_questions = await create_synthetic_dataset(products, questions_per_product=2)\n",
    "\n",
    "print(f\"Generated {len(synthetic_questions)} synthetic questions. Here is a sample:\")\n",
    "for q in random.sample(synthetic_questions, 3):\n",
    "    print(q.question)\n",
    "    print(q.required_tools)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Whether We Call The Correct Functions\n",
    "\n",
    "We'll have a function that's used to retrieve tools (so you can use it broadly), and then another function for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.16\n",
      "Recall: 0.64\n"
     ]
    }
   ],
   "source": [
    "desired_function_calls, actual_function_calls = await get_all_tool_call_evals(\n",
    "    synthetic_questions, tool_list\n",
    ")\n",
    "precision, recall = calculate_precision_recall(\n",
    "    desired_function_calls, actual_function_calls\n",
    ")\n",
    "\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cadquery",
   "language": "python",
   "name": "cadquery"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
