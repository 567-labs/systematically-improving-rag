{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Sentence Transformer Model Without Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dan/Desktop/systematically-improving-rag/systemic/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall at 5: 0.6044444444444445\n",
      "Recall at 10: 0.92\n",
      "Recall at 20: 0.9811111111111112\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6044444444444445, 0.92, 0.9811111111111112)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import lancedb\n",
    "import numpy as np\n",
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "\n",
    "# Add the week1_bootstrap_evals to path to import scoring_utils\n",
    "sys.path.append(os.path.abspath(\"../week1_bootstrap_evals\"))\n",
    "from scoring_utils import EvalQuestion\n",
    "\n",
    "model = CrossEncoder(\"cross-encoder/stsb-distilroberta-base\")\n",
    "with open(\"../week1_bootstrap_evals/synthetic_eval_dataset.json\", \"r\") as f:\n",
    "    synthetic_questions = json.load(f)\n",
    "eval_questions = [EvalQuestion(**q) for q in synthetic_questions]\n",
    "\n",
    "db = lancedb.connect(\"../week1_bootstrap_evals/lancedb\")\n",
    "reviews_table = db.open_table(\"reviews\")\n",
    "\n",
    "def score_question(eval_question: EvalQuestion):\n",
    "    query = f\"Answer the following question: {eval_question.question_with_context}\\n.\"\n",
    "    target_id = int(eval_question.chunk_id)\n",
    "    first_stage = reviews_table.search(query).limit(50).to_pandas()\n",
    "    first_stage_ids = first_stage.id.astype(int).values\n",
    "    review_text = first_stage.review.values\n",
    "    reranked_results = model.rank(query, review_text)\n",
    "    is_right_result = lambda x: first_stage_ids[x[\"corpus_id\"]] == target_id\n",
    "    try:\n",
    "        rank_of_desired_result = next(\n",
    "            i for i, d in enumerate(reranked_results) if \n",
    "            is_right_result(d)\n",
    "        )\n",
    "    except StopIteration:\n",
    "        return np.inf\n",
    "    return rank_of_desired_result\n",
    "\n",
    "ranks = [score_question(eval_question) for eval_question in eval_questions]\n",
    "recall_at_5 = np.mean([rank <= 5 for rank in ranks])\n",
    "recall_at_10 = np.mean([rank <= 10 for rank in ranks])\n",
    "recall_at_20 = np.mean([rank <= 20 for rank in ranks])\n",
    "print(f\"Recall at 5: {recall_at_5}\\nRecall at 10: {recall_at_10}\\nRecall at 20: {recall_at_20}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "systemic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
