{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start with Retrieval\n",
    "\n",
    "When building out a RAG application, it's common to start with a strong focus on evaluating the generated content. This tends to be difficult to scale and expensive to implement. Instead, we recommend starting with evaluating your retrieval.\n",
    "\n",
    "By focusing specifically on how well our retrieval system does, we can start supporting several initial use cases to start gathering feedback on early on.\n",
    "\n",
    "We can do so using retrieval metrics such as precision and recall. Precision measures the proportion of retrieved items that are relevant while recall measures the proportion of relevant items that are retrieved.\n",
    "\n",
    "These are relatively cheap and easy to compute. More importantly, they provide an objective measure of the performance of our retrieval system.\n",
    "\n",
    " \n",
    "| Aspect | Generation evals | Retrieval evals |\n",
    "|--------|------------------|-----------------|\n",
    "| Testing for | Team lead: \"Looks better now\" (based on metric evaluations that don't capture usage behavior) | Recall, precision@K |\n",
    "| Speed of tests | Slow (~1s to 10s per test) | Fast (~10ms to 800ms per test) |\n",
    "| Cost of tests | $100s per run | Negligible per run |\n",
    "| Frequency of tests | Infrequent | Frequent |\n",
    "| Speed of test iterations | Slow iterations (tests that could take minutes to hours) | Fast iterations |\n",
    "| Ease of test scalability | Difficult | Easy |\n",
    "\n",
    "This is the first in 3 notebooks where we'll be showing you how to evaluate your RAG application and systematically improve it over time. \n",
    "\n",
    "We'll be showing you how to generate synthetic questions, run simple evaluations on different configurations of retrieval before finally looking at how to visualise and reason about the results. This will be extremely useful for you to be able to iterate on your own retrieval system and improve it.\n",
    "\n",
    "\n",
    "## Synthetic Questions\n",
    "\n",
    "Since we may not have access to real user queries initially, generating synthetic questions allows us to simulate potential user interactions with our system. This approach helps us establish a performance baseline for our retrieval system and identify any retrieval issues early on, ensuring that our system is robust before deployment. \n",
    "\n",
    "This is a never-ending process. Once you do get user traffic, you'll want to keep generating more synthetic questions and start blending in user queries to make sure your system's retrieval is still doing well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Why is Text-2-SQL hard?\n",
    "\n",
    "A common use case for Language Models is to answer questions with SQL using Text-2-SQL models. These take in a user query and output a SQL query which can be used to retrieve the relevant information. \n",
    "\n",
    "```\n",
    "Text : Hey could you help me find the top 5 most popular items in our store?\n",
    "Query: SELECT item_name, COUNT(*) as popularity FROM items GROUP BY item_name ORDER BY popularity DESC LIMIT 5\n",
    "```\n",
    "\n",
    "It's common to approach this as a generation task where the solution is to keep iterating on the prompt until the model outputs the correct SQL query. \n",
    "\n",
    "This evaluation method is difficult to scale because we need to manually check the output of the sql. Additionally since a valid SQL query can be written in many ways, it's difficult to know when we have a valid solution.\n",
    "\n",
    "Instead, we can cast it as a retrieval task. Instead of measuring how good the generated sql is, we can instead measure how well our retrieval system does at retrieving relevant SQL snippets.\n",
    "\n",
    "This is important too since many companies have complex business use cases or might use specific calculation methods that are not immediately obvious to a model. Being able to nail down the retrieval of these snippets is incredibly important too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study : Bird-Bench\n",
    "\n",
    "For this case-study, we'll be using the Bird-Bench dataset. This is a huge Text-2-SQL dataset which contains a collection of text questions to a corresponding sql query. \n",
    "\n",
    "We'll be using the dev split of this dataset for this case study that provides ~1500+ sql snippets that involves ~95 different tables that we can use. \n",
    "\n",
    "We've cleaned the dataset ahead of time and uploaded it to `567-labs/bird-rag`. Each example in our dataset contains three things\n",
    "\n",
    "- `id` : This is a unique identifier for each query\n",
    "- `query` : This is a sample SQL query \n",
    "- `difficulty` : This is a label that indicates how difficult the query is to generate. It can be either `simple`, `moderate` or `challenging`. \n",
    "\n",
    "For this case study, we'll only be using the `challenging` queries so that we can generate more difficult questions. This allows us to test our retrieval system under more demanding conditions, ensuring that it performs well even with complex queries\n",
    "\n",
    "With that in mind, let's take a look at our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'query': \"SELECT `Free Meal Count (K-12)` / `Enrollment (K-12)` FROM frpm WHERE `County Name` = 'Alameda' ORDER BY (CAST(`Free Meal Count (K-12)` AS REAL) / `Enrollment (K-12)`) DESC LIMIT 1\",\n",
       " 'difficulty': 'simple'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "dataset = datasets.load_dataset(\"567-labs/bird-rag\")[\"train\"]\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we generate questions, let's take a look at what a synthetic question might look like by looking at our query below.\n",
    "\n",
    "> SELECT `Free Meal Count (K-12)` / `Enrollment (K-12)` FROM frpm WHERE `County Name` = 'Alameda' ORDER BY (CAST(`Free Meal Count (K-12)` AS REAL) / `Enrollment (K-12)`) DESC LIMIT 1\n",
    "\n",
    "\n",
    "We can see that this query is doing a few things\n",
    "\n",
    "1. First it's looking at the percentage of students that are recieving free meals \n",
    "2. It's restricting this to only schools in Alameda County\n",
    "3. Finally, it's ordering the results by this percentage in descending order and limiting the results to the top 1\n",
    "\n",
    "In short, it's trying to find the school in Alameda County with the highest percentage of students recieving free meals. So, what would be a potential question that we could ask which this query snippet would be highly relevant for?\n",
    "\n",
    "Well, a potential question could be\n",
    "\n",
    "- What is the school with the highest percentage of students recieving free meals in Alameda County?\n",
    "- What were the two schools with the highest percentage of students recieving free meals in Orange County last year?\n",
    "\n",
    "These are queries which this snippet would be highly relevant for. We want to create a dataset of similar questions that would be able to test how consistently our retrieval system does at retrieving the relevant snippet for a user query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Synthetic Questions\n",
    "\n",
    "Now let's start generating our synthetic questions. We're going to begin by defining some Pydantic models that represent the format of the data that we're working with.\n",
    "\n",
    "We're doing so because of the following reasons\n",
    "\n",
    "1. It helps us to be explicit about how and what data we're working with\n",
    "2. We can use these models with the `instructor` library to obtain structured outputs from our LLM calls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "# This represents how we're representing our data from the dataset\n",
    "class Chunk(BaseModel):\n",
    "    chunk_id: str\n",
    "    text: str\n",
    "\n",
    "\n",
    "# This is the synthetic question that we want our model to generate\n",
    "class Question(BaseModel):\n",
    "    chain_of_thought: str\n",
    "    question: str\n",
    "\n",
    "\n",
    "# This is a single question-chunk pair that we'll be uploading to Braintrust as a dataset later on to be used for benchmarking in `benchmark_retrieval.py`\n",
    "class ChunkEval(BaseModel):\n",
    "    chunk_id: str\n",
    "    question: str\n",
    "    chunk: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Synthetic Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we're using `instructor` because it handles prompt templating with `jinja` for us and provides validated structured outputs. \n",
    "\n",
    "All we need to do is to define a Pydantic model that represents a desired output and the library will handle the rest. \n",
    "\n",
    "Remember that we want to generate a question that should either be answerable by the data returned by the SQL snippet or a snippet that is similar to the SQL snippet. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the highest ratio of free meal counts to total enrollments in K-12 schools for a specific county over a recent semester?'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "import instructor\n",
    "from asyncio import Semaphore, timeout\n",
    "from tqdm.asyncio import tqdm_asyncio as asyncio\n",
    "from tenacity import retry, stop_after_attempt, wait_fixed\n",
    "\n",
    "client = instructor.from_openai(openai.OpenAI())\n",
    "\n",
    "sql_snippet = \"SELECT `Free Meal Count (K-12)` / `Enrollment (K-12)` FROM frpm WHERE `County Name` = 'Alameda' ORDER BY (CAST(`Free Meal Count (K-12)` AS REAL) / `Enrollment (K-12)`) DESC LIMIT 1\"\n",
    "\n",
    "resp = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\"\"\n",
    "        Generate a hypothetical question that can be answered using the following SQL snippet. \n",
    "\n",
    "        SQL Snippet:\n",
    "        {{ snippet }}\n",
    "\n",
    "        Rules\n",
    "        - If there are specific values in the snippet, do not use them directly in the question if possible. \n",
    "        - The question should be at most 2 sentences long\n",
    "        - if necessary, consider making the question more challenging using the following constraint - If there's a time period mentioned in the snippet, modify it slightly (Eg. if the snippet is looking at the entire year, change it to 6 months or 1.5 years)\n",
    "        - The question must be answerable using the SQL snippet or at most with a small tweak\n",
    "        \"\"\",\n",
    "        }\n",
    "    ],\n",
    "    response_model=Question,\n",
    "    context={\"snippet\": sql_snippet},\n",
    ")\n",
    "\n",
    "resp.question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ''What is the highest ratio of free meal counts to total enrollments in K-12 schools for a specific county over a recent semester?'\n",
    "\n",
    "\n",
    "This is a question which the SQL snippet would be highly relevant for. In order to answer this query, we just need to make two changes\n",
    "\n",
    "1. add in a new time filter of a recent semester\n",
    "2. change the county to a variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Up Our Questions\n",
    "\n",
    "Now that we've seen how to generate a single question, let's scale this up and generate more questions using the same approach.\n",
    "\n",
    "We'll do so by generating a question for each SQL snippet marked as challenging. Since this will be a large number of requests, we're going to be doing so asynchronously with the `asyncio` library.\n",
    "\n",
    "Additionally, to make sure we stay within our rate limits , we'll be using a semaphore to limit the number of concurrent requests.\n",
    "\n",
    "We're also making sure that we have a good diversity of questions by randomly selecting a constraint from a set of constraints to make the question more challenging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [01:02<00:00,  4.62it/s]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Define Instructor Client\n",
    "client = instructor.from_openai(openai.AsyncOpenAI())\n",
    "\n",
    "# Define some constraints to make the question more challenging\n",
    "constraints = [\n",
    "    \"If there's a time period mentioned in the snippet, modify it slightly (Eg. if the snippet is looking at the entire year, change it to 6 months or 1.5 years)\",\n",
    "    \"Add in some irrelevant context (Eg. Add information about the weather, a random event or a backstory that isn't mentioned in the snippet)\",\n",
    "    \"Changing the value of the filter (Eg. if the snippet is looking at the results in Canada, change the question to ask about another country or city instead)\",\n",
    "]\n",
    "\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_fixed(10))\n",
    "async def generate_questions(chunk: Chunk, sem: Semaphore) -> ChunkEval:\n",
    "    async with sem, timeout(30):\n",
    "        resp = await client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"\"\"\n",
    "                Generate a hypothetical question that can be answered using the following SQL snippet. \n",
    "\n",
    "                SQL Snippet:\n",
    "                {{ snippet }}\n",
    "\n",
    "                Rules\n",
    "                - If there are specific values in the snippet, do not use them directly in the question if possible. \n",
    "                - The question should be at most 2 sentences long\n",
    "                - if necessary, consider making the question more challenging using the following constraint - {{ constraint }}\n",
    "                - The question must be answerable using the SQL snippet or at most with a small tweak\n",
    "                \"\"\",\n",
    "                }\n",
    "            ],\n",
    "            response_model=Question,\n",
    "            context={\"snippet\": chunk.text, \"constraint\": random.choice(constraints)},\n",
    "            timeout=20,\n",
    "        )\n",
    "\n",
    "        return ChunkEval(\n",
    "            chunk_id=chunk.chunk_id,\n",
    "            question=resp.question,\n",
    "            chunk=chunk.text,\n",
    "        )\n",
    "\n",
    "\n",
    "sem = Semaphore(10)\n",
    "dataset = [\n",
    "    item\n",
    "    for item in datasets.load_dataset(\"567-labs/bird-rag\")[\"train\"]\n",
    "    if item[\"difficulty\"] == \"challenging\"\n",
    "]\n",
    "dataset = [Chunk(chunk_id=item[\"id\"], text=item[\"query\"]) for item in dataset]\n",
    "\n",
    "coros = []\n",
    "\n",
    "num_samples = 2\n",
    "for chunk in dataset:\n",
    "    for _ in range(num_samples):\n",
    "        coros.append(generate_questions(chunk, sem))\n",
    "\n",
    "questions: list[ChunkEval] = await asyncio.gather(*coros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've generated our questions, let's take a look at what they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "    Question: In the past year, which locally funded schools have an enrollment difference between K-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> and ages \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span> that exceeds the average difference among their peers?\n",
       "\n",
       "    SQL Snippet: SELECT T2.School, T2.DOC FROM frpm AS T1 INNER JOIN schools AS T2 ON T1.CDSCode = T2.CDSCode WHERE\n",
       "T2.FundingType = <span style=\"color: #008000; text-decoration-color: #008000\">'Locally funded'</span> AND <span style=\"font-weight: bold\">(</span>T1.`Enrollment <span style=\"font-weight: bold\">(</span>K-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span><span style=\"font-weight: bold\">)</span>` - T1.`Enrollment <span style=\"font-weight: bold\">(</span>Ages <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span><span style=\"font-weight: bold\">)</span>`<span style=\"font-weight: bold\">)</span> &gt; <span style=\"font-weight: bold\">(</span>SELECT \n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AVG</span><span style=\"font-weight: bold\">(</span>T3.`Enrollment <span style=\"font-weight: bold\">(</span>K-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span><span style=\"font-weight: bold\">)</span>` - T3.`Enrollment <span style=\"font-weight: bold\">(</span>Ages <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span><span style=\"font-weight: bold\">)</span>`<span style=\"font-weight: bold\">)</span> FROM frpm AS T3 INNER JOIN schools AS T4 ON T3.CDSCode = \n",
       "T4.CDSCode WHERE T4.FundingType = <span style=\"color: #008000; text-decoration-color: #008000\">'Locally funded'</span><span style=\"font-weight: bold\">)</span>\n",
       "    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "    Question: In the past year, which locally funded schools have an enrollment difference between K-\u001b[1;36m12\u001b[0m and ages \n",
       "\u001b[1;36m5\u001b[0m-\u001b[1;36m17\u001b[0m that exceeds the average difference among their peers?\n",
       "\n",
       "    SQL Snippet: SELECT T2.School, T2.DOC FROM frpm AS T1 INNER JOIN schools AS T2 ON T1.CDSCode = T2.CDSCode WHERE\n",
       "T2.FundingType = \u001b[32m'Locally funded'\u001b[0m AND \u001b[1m(\u001b[0mT1.`Enrollment \u001b[1m(\u001b[0mK-\u001b[1;36m12\u001b[0m\u001b[1m)\u001b[0m` - T1.`Enrollment \u001b[1m(\u001b[0mAges \u001b[1;36m5\u001b[0m-\u001b[1;36m17\u001b[0m\u001b[1m)\u001b[0m`\u001b[1m)\u001b[0m > \u001b[1m(\u001b[0mSELECT \n",
       "\u001b[1;35mAVG\u001b[0m\u001b[1m(\u001b[0mT3.`Enrollment \u001b[1m(\u001b[0mK-\u001b[1;36m12\u001b[0m\u001b[1m)\u001b[0m` - T3.`Enrollment \u001b[1m(\u001b[0mAges \u001b[1;36m5\u001b[0m-\u001b[1;36m17\u001b[0m\u001b[1m)\u001b[0m`\u001b[1m)\u001b[0m FROM frpm AS T3 INNER JOIN schools AS T4 ON T3.CDSCode = \n",
       "T4.CDSCode WHERE T4.FundingType = \u001b[32m'Locally funded'\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "    Question: Which schools in France are locally funded and have a greater difference between their total K-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span> \n",
       "enrollment and enrollment for ages <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span> compared to the average difference for locally funded schools?\n",
       "\n",
       "    SQL Snippet: SELECT T2.School, T2.DOC FROM frpm AS T1 INNER JOIN schools AS T2 ON T1.CDSCode = T2.CDSCode WHERE\n",
       "T2.FundingType = <span style=\"color: #008000; text-decoration-color: #008000\">'Locally funded'</span> AND <span style=\"font-weight: bold\">(</span>T1.`Enrollment <span style=\"font-weight: bold\">(</span>K-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span><span style=\"font-weight: bold\">)</span>` - T1.`Enrollment <span style=\"font-weight: bold\">(</span>Ages <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span><span style=\"font-weight: bold\">)</span>`<span style=\"font-weight: bold\">)</span> &gt; <span style=\"font-weight: bold\">(</span>SELECT \n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AVG</span><span style=\"font-weight: bold\">(</span>T3.`Enrollment <span style=\"font-weight: bold\">(</span>K-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span><span style=\"font-weight: bold\">)</span>` - T3.`Enrollment <span style=\"font-weight: bold\">(</span>Ages <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span><span style=\"font-weight: bold\">)</span>`<span style=\"font-weight: bold\">)</span> FROM frpm AS T3 INNER JOIN schools AS T4 ON T3.CDSCode = \n",
       "T4.CDSCode WHERE T4.FundingType = <span style=\"color: #008000; text-decoration-color: #008000\">'Locally funded'</span><span style=\"font-weight: bold\">)</span>\n",
       "    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "    Question: Which schools in France are locally funded and have a greater difference between their total K-\u001b[1;36m12\u001b[0m \n",
       "enrollment and enrollment for ages \u001b[1;36m5\u001b[0m-\u001b[1;36m17\u001b[0m compared to the average difference for locally funded schools?\n",
       "\n",
       "    SQL Snippet: SELECT T2.School, T2.DOC FROM frpm AS T1 INNER JOIN schools AS T2 ON T1.CDSCode = T2.CDSCode WHERE\n",
       "T2.FundingType = \u001b[32m'Locally funded'\u001b[0m AND \u001b[1m(\u001b[0mT1.`Enrollment \u001b[1m(\u001b[0mK-\u001b[1;36m12\u001b[0m\u001b[1m)\u001b[0m` - T1.`Enrollment \u001b[1m(\u001b[0mAges \u001b[1;36m5\u001b[0m-\u001b[1;36m17\u001b[0m\u001b[1m)\u001b[0m`\u001b[1m)\u001b[0m > \u001b[1m(\u001b[0mSELECT \n",
       "\u001b[1;35mAVG\u001b[0m\u001b[1m(\u001b[0mT3.`Enrollment \u001b[1m(\u001b[0mK-\u001b[1;36m12\u001b[0m\u001b[1m)\u001b[0m` - T3.`Enrollment \u001b[1m(\u001b[0mAges \u001b[1;36m5\u001b[0m-\u001b[1;36m17\u001b[0m\u001b[1m)\u001b[0m`\u001b[1m)\u001b[0m FROM frpm AS T3 INNER JOIN schools AS T4 ON T3.CDSCode = \n",
       "T4.CDSCode WHERE T4.FundingType = \u001b[32m'Locally funded'\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich import print\n",
    "\n",
    "\n",
    "for i in range(2):\n",
    "    print(f\"\"\"\n",
    "    Question: {questions[i].question}\n",
    "\n",
    "    SQL Snippet: {questions[i].chunk}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at both of the generated questions, they're essentially asking about the same thing - that is the schools that are locally funded and have an enrollment difference that's above average.\n",
    "\n",
    "However, despite so, they have different wording.\n",
    "\n",
    "The first has a time frame of 1 year while the second one is looking at schools in France specifically. This is a small change but it's enough to create diversity in our questions.\n",
    "\n",
    "We can scale this up further by adding more constraints and generating more questions. This is crucial in uncovering blindspots in our retrieval system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading our Dataset\n",
    "\n",
    "We use braintrust because it's easy to track different experiments, allows us to run multiple experiments and trials at once and provides an easy way to store private datasets.\n",
    "\n",
    "In this case, we're going to be uploading our synthetic questions to braintrust. We'll be doing so under the `Text-2-SQL` project and calling our dataset `Bird-Bench-Questions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DatasetSummary</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">project_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Text-2-SQL'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">dataset_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Bird-Bench-Questions'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">project_url</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'https://www.braintrust.dev/app/567/p/Text-2-SQL'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">dataset_url</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'https://www.braintrust.dev/app/567/p/Text-2-SQL/datasets/Bird-Bench-Questions'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">data_summary</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DataSummary</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">new_records</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">290</span>, <span style=\"color: #808000; text-decoration-color: #808000\">total_records</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">290</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDatasetSummary\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mproject_name\u001b[0m=\u001b[32m'Text-2-SQL'\u001b[0m,\n",
       "    \u001b[33mdataset_name\u001b[0m=\u001b[32m'Bird-Bench-Questions'\u001b[0m,\n",
       "    \u001b[33mproject_url\u001b[0m=\u001b[32m'https://www.braintrust.dev/app/567/p/Text-2-SQL'\u001b[0m,\n",
       "    \u001b[33mdataset_url\u001b[0m=\u001b[32m'https://www.braintrust.dev/app/567/p/Text-2-SQL/datasets/Bird-Bench-Questions'\u001b[0m,\n",
       "    \u001b[33mdata_summary\u001b[0m=\u001b[1;35mDataSummary\u001b[0m\u001b[1m(\u001b[0m\u001b[33mnew_records\u001b[0m=\u001b[1;36m290\u001b[0m, \u001b[33mtotal_records\u001b[0m=\u001b[1;36m290\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import braintrust\n",
    "\n",
    "\n",
    "dataset = braintrust.init_dataset(project=\"Text-2-SQL\", name=\"Bird-Bench-Questions\")\n",
    "for question in questions:\n",
    "    dataset.insert(\n",
    "        input=question.question,\n",
    "        expected=[question.chunk],\n",
    "        metadata={\"chunk_id\": question.chunk_id, \"chunk\": question.chunk},\n",
    "    )\n",
    "\n",
    "print(dataset.summarize())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
