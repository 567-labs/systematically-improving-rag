{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal\n",
    "\n",
    "This notebook shows how to make synthetic data to bootstrap evaluation of your retrieval system. \n",
    "\n",
    "This synthetic data contains many triplets of `(RAG system input, system output, desired chunk to retrieve)`. For this example, we will work on a hardware retailer's system to answer user questions based on existing product previous. So the synthetic data will look like\n",
    "\n",
    "```\n",
    "Q: How frequently do I need to replace the blades on this saw?\n",
    "A: A customer reported getting 7-10 hours of active use between blade replacements.\n",
    "Chunk ID: 3\n",
    "```\n",
    "\n",
    "Once you have many of these triplets, you can experiments with different retrieval strategies (e.g. different embedding models, embedding vs keyword search, etc) to determine which strategies most consistently retrieve the desired chunks.\n",
    "\n",
    "# A Starting Point\n",
    "\n",
    "A simple approach would follow pseudo code:\n",
    "\n",
    "```\n",
    "synth_data = []\n",
    "for chunk in corpus:\n",
    "    response = call_llm(f\"Give me a JSON array of 10 question/answer pairs. The questions should be things someone might ask about a product before purchase. The answer should be something contained in this text: {chunk}\")\n",
    "    q_a = json.loads(response.content)\n",
    "    q_a_c = [{'question': q, 'answer': a, 'chunk': chunk} for (q, a) in q_a_pairs]\n",
    "    synth_data.extend(q_a_c)\n",
    "```\n",
    "\n",
    "A practical implementation should address three issues that arise in the naive pseudo code.\n",
    "\n",
    "| Issue | Solution |\n",
    "|---------|----------|\n",
    "| Inconsistent formatting of LLM response (e.g. different keys) | Instructor library |\n",
    "| Bad questions | Guidance/examples in prompt |\n",
    "| Time waiting for LLM responses when iterating over many chunks | Async LLM calls|\n",
    "\n",
    "# Reusable Code to Bootstrap Evals\n",
    "\n",
    "The code in this notebook addresses these issues. The code is also available as [this script](https://gist.github.com/jxnl/5627c9d463ffe0b085896f7890fab1bf).\n",
    "\n",
    "## Data\n",
    "\n",
    "This course uses synthetic data based on the use-case of answering questions on a hardware retailer's website based on product reviews. We have created this data in `make_product_reviews.ipynb`. Here is a small sample of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      I've been using this hammer for a few months now, and it's become my go-to tool for all my DIY projects. The 16 oz weight is perfect for driving nails witho...\n",
       "1      This hammer is a solid addition to my toolbox. The balance between the handle and the head makes it easy to control, and the 16 oz weight is just right for ...\n",
       "2      I purchased this hammer for some home renovation work, and it has exceeded my expectations. The steel head is tough and has withstood a lot of heavy use wit...\n",
       "3      As a professional carpenter, I rely on my tools daily, and this hammer has not disappointed. The 16 oz weight is perfect for driving nails quickly and effic...\n",
       "4      This hammer is a great value for the price. The 16 oz weight is perfect for general carpentry and DIY projects. The grip is comfortable and doesn't slip, ev...\n",
       "                                                                                    ...                                                                               \n",
       "895    I've tried several tool belts over the years, and this one is by far the best. The breathable mesh design keeps me cool, and the lightweight construction is...\n",
       "896    This tool belt is a must-have for any serious DIYer. The breathable mesh design is perfect for working in hot conditions, and the lightweight material doesn...\n",
       "897    I purchased this tool belt for my son, who is an apprentice electrician, and he loves it. The breathable mesh design keeps him cool, and the lightweight con...\n",
       "898    This tool belt is a great investment. The breathable mesh design is perfect for hot weather, and the lightweight material makes it comfortable to wear all d...\n",
       "899    I've been using this tool belt for a while now, and it's been a great addition to my toolkit. The breathable mesh design keeps me cool, and the lightweight ...\n",
       "Name: review, Length: 900, dtype: object"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lancedb\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 160)\n",
    "\n",
    "db = lancedb.connect(\"./lancedb\")\n",
    "reviews_table = db.open_table(\"reviews\")\n",
    "sample_reviews = reviews_table.to_pandas()\n",
    "sample_reviews.review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure The Data\n",
    "\n",
    "We use Pydantic & Instructor for a reliable interface between our LLMs and the structured data formats we need to run code on LLM output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Review(BaseModel):\n",
    "    id: str\n",
    "    product_title: str\n",
    "    product_description: str\n",
    "    review: str\n",
    "\n",
    "\n",
    "sample_chunks = [\n",
    "    Review(\n",
    "        id=str(row.id),\n",
    "        product_title=row.product_title,\n",
    "        product_description=row.product_description,\n",
    "        review=row.review,\n",
    "    )\n",
    "    for _, row in sample_reviews.iterrows()\n",
    "]\n",
    "\n",
    "n_questions = 2  # number of questions to get in each LLM call\n",
    "example_questions = [\n",
    "    \"What does the reviewer like about the product?\",\n",
    "    \"What does the reviewer think could be improved?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now see how we build questions on a single chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dan/Desktop/systematically-improving-rag/systemic/lib/python3.10/site-packages/pydantic/_internal/_config.py:341: UserWarning: Valid config keys have changed in V2:\n",
      "* 'allow_population_by_field_name' has been renamed to 'populate_by_name'\n",
      "* 'smart_union' has been removed\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ChunkEval(question='How heavy is the hammer and is it good for driving nails?', answer='The hammer weighs 16 oz, which is perfect for driving nails without too much effort.', chunk_id='0', question_with_context='A user asked the following question:\\nQuestion: How heavy is the hammer and is it good for driving nails?\\nThis is about the following product:\\nProduct Title: Hammer\\nProduct Description: This 16 oz claw hammer is perfect for general carpentry and DIY projects. It features a comfortable grip and a durable steel head.\\n'),\n",
       " ChunkEval(question='Is the grip comfortable for long use?', answer=\"Yes, the grip is comfortable even during extended use, and there hasn't been any noticeable wear on it.\", chunk_id='0', question_with_context='A user asked the following question:\\nQuestion: Is the grip comfortable for long use?\\nThis is about the following product:\\nProduct Title: Hammer\\nProduct Description: This 16 oz claw hammer is perfect for general carpentry and DIY projects. It features a comfortable grip and a durable steel head.\\n')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "import instructor\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "# Patch the AsyncOpenAI client\n",
    "client = instructor.from_openai(AsyncOpenAI())\n",
    "\n",
    "\n",
    "class QuestionAnswer(BaseModel):\n",
    "    question: str\n",
    "    answer: str\n",
    "\n",
    "\n",
    "class ChunkEval(QuestionAnswer):\n",
    "    chunk_id: str\n",
    "    question_with_context: str\n",
    "\n",
    "async def generate_evals(\n",
    "    review: Review, n_questions: int, example_questions: List[str]\n",
    ") -> List[ChunkEval]:\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "        Generate `{n_questions}` question-answer pairs about a {review.product_title}. The answers should primarily be derived from information in this product review:\n",
    "\n",
    "        <content>\n",
    "        {review.review}\n",
    "        </content>\n",
    "\n",
    "        While they should contain information from the product review, you may also find it helpful context to see a product description:\n",
    "        <content>\n",
    "        {review.product_description}\n",
    "        </content>\n",
    "\n",
    "        Example questions:\n",
    "        {chr(10).join(f'- {q}' for q in example_questions)}\n",
    "\n",
    "        Provide a concise and specific answer for each question.\n",
    "        Do not use the exact example questions. Use them only as inspiration for the types of more specific questions to generate.\n",
    "        Do not include answers that are not in the content.\n",
    "        Questions should ask about product characteristics (e.g. durability) and answers should refer to product characteristics without referring to the reviewer specifically.\n",
    "        Stylistically, the questions should resemble what people would ask a RAG-based answer bot on a retailer's website. So they can be a little informal, messy or scattered.\n",
    "        \"\"\"\n",
    "\n",
    "    def make_context(question: str) -> str:\n",
    "        return f\"\"\"A user asked the following question:\n",
    "Question: {question}\n",
    "This is about the following product:\n",
    "Product Title: {review.product_title}\n",
    "Product Description: {review.product_description}\n",
    "\"\"\"\n",
    "    try:\n",
    "        pairs = client.chat.completions.create_iterable(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            response_model=QuestionAnswer,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.0,\n",
    "        )\n",
    "\n",
    "        return [\n",
    "            ChunkEval(\n",
    "                question=pair.question,\n",
    "                answer=pair.answer,\n",
    "                chunk_id=review.id,\n",
    "                question_with_context=make_context(pair.question),\n",
    "            )\n",
    "            async for pair in pairs\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating evals: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "first_chunk_res = await generate_evals(sample_chunks[0], n_questions, example_questions)\n",
    "first_chunk_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run `generate_evals` for many chunks in parallel, wrap it with a function that also takes a semaphore. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChunkEval(question='How heavy is the hammer and is it good for driving nails?', answer='The hammer weighs 16 oz, which is perfect for driving nails without too much effort.', chunk_id='0', question_with_context='A user asked the following question:\\nQuestion: How heavy is the hammer and is it good for driving nails?\\nThis is about the following product:\\nProduct Title: Hammer\\nProduct Description: This 16 oz claw hammer is perfect for general carpentry and DIY projects. It features a comfortable grip and a durable steel head.\\n'),\n",
       " ChunkEval(question='Is the grip comfortable for long use?', answer='Yes, the grip is comfortable even during extended use, and there has been no noticeable wear on it.', chunk_id='0', question_with_context='A user asked the following question:\\nQuestion: Is the grip comfortable for long use?\\nThis is about the following product:\\nProduct Title: Hammer\\nProduct Description: This 16 oz claw hammer is perfect for general carpentry and DIY projects. It features a comfortable grip and a durable steel head.\\n')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "class ChunkProcessingError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "async def process_chunk(\n",
    "    review: Review,\n",
    "    n_questions: int,\n",
    "    example_questions: List[str],\n",
    "    semaphore: asyncio.Semaphore,\n",
    ") -> List[ChunkEval]:\n",
    "    async with semaphore:\n",
    "        try:\n",
    "            return await generate_evals(review, n_questions, example_questions)\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error processing chunk {review.id}: {str(e)}\")\n",
    "            raise ChunkProcessingError(f\"Failed to process chunk {review.id}\") from e\n",
    "\n",
    "\n",
    "# Test that we get the same results as directly calling generate_evals\n",
    "await process_chunk(\n",
    "    sample_chunks[0], n_questions, example_questions, asyncio.Semaphore(1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can call `process_chunks` with all chunks to build the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1800 ChunkEvals.\n",
      "Saved 900 samples as 'synthetic_eval_dataset.json'\n",
      "Saved 900 samples as 'synthetic_finetune_dataset.jsonl'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "async def create_synthetic_dataset(\n",
    "    reviews: List[Review],\n",
    "    n_questions: int,\n",
    "    example_questions: List[str],\n",
    "    max_concurrency: int = 10,\n",
    ") -> List[ChunkEval]:\n",
    "    semaphore = asyncio.Semaphore(max_concurrency)\n",
    "    tasks = [\n",
    "        process_chunk(review, n_questions, example_questions, semaphore)\n",
    "        for review in reviews\n",
    "    ]\n",
    "    results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "\n",
    "    dataset = []\n",
    "    for result in results:\n",
    "        if isinstance(result, ChunkProcessingError):\n",
    "            print(result)\n",
    "        elif isinstance(result, list):\n",
    "            dataset.extend(result)\n",
    "        else:\n",
    "            print(f\"Unexpected result type: {type(result)}\")\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def save_eval_data(dataset: List[ChunkEval], filename: str):\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump([chunk_eval.model_dump() for chunk_eval in dataset], f, indent=2)\n",
    "\n",
    "def save_ft_data(dataset: List[ChunkEval], filename: str):\n",
    "    reviews_df = reviews_table.to_pandas()\n",
    "    with open(filename, \"w\") as f:\n",
    "        for chunk_eval in dataset:\n",
    "            review_text = reviews_df.iloc[int(chunk_eval.chunk_id)].review\n",
    "            json_object = {\n",
    "                'query': chunk_eval.question_with_context,\n",
    "                'relevant_passages': [review_text],\n",
    "            }\n",
    "            f.write(json.dumps(json_object) + '\\n')\n",
    "\n",
    "synthetic_dataset = await create_synthetic_dataset(\n",
    "    sample_chunks, n_questions, example_questions\n",
    ")\n",
    "\n",
    "# Randomly shuffle the dataset\n",
    "random.shuffle(synthetic_dataset)\n",
    "\n",
    "# Split the dataset into two halves\n",
    "split_index = len(synthetic_dataset) // 2\n",
    "eval_dataset = synthetic_dataset[:split_index]\n",
    "finetune_dataset = synthetic_dataset[split_index:]\n",
    "\n",
    "# Save the two datasets\n",
    "save_eval_data(eval_dataset, \"synthetic_eval_dataset.json\")\n",
    "save_ft_data(finetune_dataset, \"synthetic_finetune_dataset.jsonl\")\n",
    "\n",
    "print(f\"Generated {len(synthetic_dataset)} ChunkEvals.\")\n",
    "print(f\"Saved {len(eval_dataset)} samples as 'synthetic_eval_dataset.json'\")\n",
    "print(f\"Saved {len(finetune_dataset)} samples as 'synthetic_finetune_dataset.jsonl'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "systemic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
