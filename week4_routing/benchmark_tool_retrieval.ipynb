{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Tool Call Choice\n",
    "\n",
    "You can build many tools. But calling all tools for all queries would be slow (and possibly harmful if they have side effects).\n",
    "\n",
    "We benchmark tool retrieval with synthetic questions much like how we measured document retrieval in week 1. This section shows how to create the tool retrieval benchmark, and we'll improve retrieval in week 4.\n",
    "\n",
    "\n",
    "## Load Products\n",
    "\n",
    "We will test tool recall on a sample of our product inventory. Weload these products below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dan/.pyenv/versions/3.10.0/lib/python3.10/site-packages/pydantic/_internal/_config.py:334: UserWarning: Valid config keys have changed in V2:\n",
      "* 'allow_population_by_field_name' has been renamed to 'populate_by_name'\n",
      "* 'smart_union' has been removed\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Product(title='Level', description='A 9-inch torpedo level that fits easily in your toolbox. The magnetic strip allows for hands-free use on metal surfaces.'),\n",
       " Product(title='Nail Gun', description='This cordless nail gun offers the convenience of battery power. The anti-jam mechanism ensures smooth operation.'),\n",
       " Product(title='Air Compressor', description='A portable 3-gallon air compressor with a built-in handle for easy transport. The quick-connect coupler allows for fast tool changes.')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "from typing import List, Union\n",
    "from pydantic import BaseModel, Field\n",
    "import instructor\n",
    "from openai import AsyncOpenAI\n",
    "import lancedb\n",
    "import random\n",
    "\n",
    "\n",
    "class Product(BaseModel):\n",
    "    title: str\n",
    "    description: str\n",
    "\n",
    "\n",
    "try:\n",
    "    db = lancedb.connect(\"../week1_bootstrap_evals/lancedb\")\n",
    "    products = db.open_table(\"products\").to_pandas()\n",
    "    products = [\n",
    "        Product(title=row[\"title\"], description=row[\"description\"])\n",
    "        for _, row in products.iterrows()\n",
    "    ]\n",
    "except Exception as e:\n",
    "    print(\n",
    "        f\"Error loading product data. Run the week1 course notebooks first to create the products DB\"\n",
    "    )\n",
    "    print(f\"Error: {str(e)}\")\n",
    "\n",
    "random.sample(products, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Types for Tool Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShippingDateRequest(BaseModel):\n",
    "    '''Check when a product will be shipped'''\n",
    "    call_name: str = \"ShippingDateRequest\"\n",
    "    sku: str = Field(..., description=\"SKU of the product to check shipping date for\")\n",
    "\n",
    "\n",
    "class ShippingCostRequest(BaseModel):\n",
    "    '''Check the cost of shipping a product'''\n",
    "    sku: str = Field(..., description=\"SKU of the product to check shipping cost for\")\n",
    "    shipping_location: str = Field(..., description=\"Location to ship to\")\n",
    "\n",
    "class ProductDimensionsRequest(BaseModel):\n",
    "    '''Check the dimensions of a product'''\n",
    "    sku: str = Field(..., description=\"SKU of the product to check dimensions for\")\n",
    "\n",
    "\n",
    "class PriceHistoryRequest(BaseModel):\n",
    "    '''Check the price history of a product (e.g. identifying historical price fluctuations)'''\n",
    "    sku: str = Field(..., description=\"SKU of the product to check price history for\")\n",
    "\n",
    "\n",
    "class ProductComparisonRequest(BaseModel):\n",
    "    '''Compare two products'''\n",
    "    sku1: str = Field(..., description=\"SKU of the first product to compare\")\n",
    "    sku2: str = Field(..., description=\"SKU of the second product to compare\")\n",
    "\n",
    "\n",
    "class LogDesiredFeatureRequest(BaseModel):\n",
    "    '''Record a user's desire for a certain product feature'''\n",
    "    sku: str = Field(..., description=\"SKU of the product to log a desired feature for\")\n",
    "    user_id: str = Field(..., description=\"User ID to log the desired feature for\")\n",
    "    desired_feature: str = Field(..., description=\"Desired feature to log\")\n",
    "\n",
    "\n",
    "class ExtractDataFromImageRequest(BaseModel):\n",
    "    '''Use our product images with multimodal llm to extract info about the product'''\n",
    "    image_url: str = Field(..., description=\"URL of the image to examine\")\n",
    "    question: str = Field(..., description=\"Question to answer about the image\")\n",
    "\n",
    "\n",
    "class ProductMaterialsRequest(BaseModel):\n",
    "    '''Check what materials a product is made of'''\n",
    "    sku: str = Field(..., description=\"SKU of the product to check materials for\")\n",
    "\n",
    "\n",
    "FunctionOption = Union[\n",
    "    ShippingDateRequest,\n",
    "    ShippingCostRequest,\n",
    "    ProductDimensionsRequest,\n",
    "    PriceHistoryRequest,\n",
    "    ProductComparisonRequest,\n",
    "    LogDesiredFeatureRequest,\n",
    "    ExtractDataFromImageRequest,\n",
    "    ProductMaterialsRequest,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Synthetic Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 180 synthetic questions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[QuestionWithTools(question='Has the price of this pliers set been stable? Also, what are they made of?', required_tools=FunctionList(func_names=['PriceHistoryRequest', 'ProductMaterialsRequest']), product=Product(title='Pliers Set', description='This 3-piece pliers set is perfect for household repairs. The cushioned grips provide comfort and control.')),\n",
       " QuestionWithTools(question=\"Will this fit in my toolbox? It's about 15 inches long and 5 inches wide.\", required_tools=FunctionList(func_names=['ProductDimensionsRequest', 'ExtractDataFromImageRequest']), product=Product(title='Caulking Gun', description='A lightweight caulking gun with a built-in cutter and seal punch. The ladder hook provides convenient storage.')),\n",
       " QuestionWithTools(question='If I order the tape measure today, how much will shipping cost and when will it arrive?', required_tools=FunctionList(func_names=['ShippingDateRequest', 'ShippingCostRequest']), product=Product(title='Tape Measure', description='A 25-foot tape measure with a durable, non-slip casing. The easy-to-read markings make it simple to take accurate measurements.'))]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "async_client = instructor.from_openai(AsyncOpenAI())\n",
    "\n",
    "def describe_tools(tools: List[FunctionOption]) -> str:\n",
    "    return \"\\n\".join([f\"{tool.__name__}: {tool.__doc__}\" for tool in tools])\n",
    "\n",
    "class FunctionList(BaseModel):\n",
    "    func_names: List[str]\n",
    "\n",
    "class QuestionWithTools(BaseModel):\n",
    "    question: str\n",
    "    required_tools: FunctionList\n",
    "    product: Product\n",
    "\n",
    "\n",
    "def random_tool_selection() -> List[FunctionOption]:\n",
    "    num_tools = random.choice([0, 1, 2])\n",
    "    return random.sample(FunctionOption.__args__, num_tools)\n",
    "\n",
    "async def generate_synthetic_question(product: Product) -> QuestionWithTools:\n",
    "    tools_to_use = random_tool_selection()\n",
    "    prompt = f\"\"\"\n",
    "    Create a realistic question a customer might ask a support chatbot about this product:\n",
    "    {product.title}: {product.description}\n",
    "\n",
    "    The customer knows this is a programmatic chatbot. So they will be terse and lazy (possibly skipping whole/fully formed sentences).\n",
    "    \"\"\"\n",
    "    if tools_to_use:\n",
    "        prompt += f\"\"\"The question should require using these function calls: {describe_tools(tools_to_use)}\n",
    "    \n",
    "    Do not explicitly ask for the function. Instead, ask a question that happens to answerable by calling the function.\n",
    "\n",
    "    For example:\n",
    "    Instead of asking `how long shipping will take`, say `I need it by Friday. Can you make it?`\n",
    "    Instead of asking for product dimensions, ask `Would this fit in a 3x7x4 case?`\n",
    "    Instead of asking for the price history, ask `Is now a good time to buy?`\n",
    "\n",
    "    Real questions tend to be implicit.\n",
    "    Ask questions where it is hard to identify what tool(s) would help an LLM to answer the question.\n",
    "    Assume that we will not make a tool call to look something up if it is already in the product description.\n",
    "\n",
    "    Respond with the question.\n",
    "    \"\"\"\n",
    "    else:\n",
    "        prompt += f\"\"\"Respond with a question that can be answered without calling any of these tools:\n",
    "        {describe_tools(FunctionOption.__args__)}\n",
    "        \"\"\"\n",
    "\n",
    "    question = await async_client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are creating synthetic questions for benchmarking tool retrieval in a retail chatbot.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        response_model=str,\n",
    "        temperature=0.,\n",
    "    )\n",
    "    tools_names = FunctionList(func_names=[tool.__name__ for tool in tools_to_use])\n",
    "    return QuestionWithTools(\n",
    "        question=question, required_tools=tools_names, product=product\n",
    "    )\n",
    "\n",
    "\n",
    "async def create_synthetic_dataset(products: List[Product], questions_per_product: int) -> List[QuestionWithTools]:\n",
    "    tasks = [\n",
    "        generate_synthetic_question(product)\n",
    "        for product in products\n",
    "        for _ in range(questions_per_product)\n",
    "    ]\n",
    "    return await asyncio.gather(*tasks)\n",
    "\n",
    "\n",
    "synthetic_questions = await create_synthetic_dataset(products, questions_per_product=2)\n",
    "print(f\"Generated {len(synthetic_questions)} synthetic questions\")\n",
    "\n",
    "random.sample(synthetic_questions, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Whether Calling Correct Functions\n",
    "\n",
    "We'll have a function that's used to retrieve tools (so you can use it broadly), and then another function for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I need a hammer soon. If I order today, when will it arrive? Also, how does this hammer compare to the 20 oz claw hammer you have?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FunctionList(func_names=['ShippingDateRequest', 'ProductComparisonRequest'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "async def choose_tools(question: str, product: Product) -> FunctionList:\n",
    "    try:\n",
    "        response = await async_client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": f\"\"\"Identify the tools that will help you answer the user's question? \n",
    "                    Respond with the names of 0, 1 or 2 tools to use. The available tools are\n",
    "                    {describe_tools(FunctionOption.__args__)}.\n",
    "\n",
    "                    For context, the user's question is about the following product:\n",
    "                    {product.title}: {product.description}\n",
    "\n",
    "                    Don't make function calls to look up information that is already in the product description.\n",
    "                    \"\"\",\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": question},\n",
    "            ],\n",
    "            temperature=0.0,\n",
    "            response_model=FunctionList,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error in API call: {str(e)}\")\n",
    "    return response\n",
    "\n",
    "example_question = synthetic_questions[0]\n",
    "print(example_question.question)\n",
    "await choose_tools(example_question.question, example_question.product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToolCallEvaluation(BaseModel):\n",
    "    question: str\n",
    "    expected: FunctionList\n",
    "    predicted: FunctionList\n",
    "\n",
    "\n",
    "async def get_tool_call_evals(q: QuestionWithTools) -> ToolCallEvaluation:\n",
    "    predicted = await choose_tools(q.question, q.product)\n",
    "    return ToolCallEvaluation(\n",
    "        question=q.question,\n",
    "        expected=q.required_tools,\n",
    "        predicted=predicted,\n",
    "    )\n",
    "\n",
    "\n",
    "async def run_evaluation(\n",
    "    synthetic_questions: List[QuestionWithTools], max_concurrency: int = 40\n",
    ") -> List[ToolCallEvaluation]:\n",
    "    semaphore = asyncio.Semaphore(max_concurrency)\n",
    "\n",
    "    async def bounded_get_tool_call_evals(q: QuestionWithTools):\n",
    "        async with semaphore:\n",
    "            return await get_tool_call_evals(q)\n",
    "\n",
    "    tasks = [bounded_get_tool_call_evals(q) for q in synthetic_questions]\n",
    "    return await asyncio.gather(*tasks)\n",
    "\n",
    "\n",
    "evaluation_results = await run_evaluation(synthetic_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.71\n",
      "Recall: 0.97\n"
     ]
    }
   ],
   "source": [
    "def calculate_precision_recall(evaluation_results: List[ToolCallEvaluation]):\n",
    "    true_positives = sum(\n",
    "        len(\n",
    "            set(result.expected.func_names).intersection(\n",
    "                set(result.predicted.func_names)\n",
    "            )\n",
    "        )\n",
    "        for result in evaluation_results\n",
    "    )\n",
    "    false_positives = sum(\n",
    "        len(set(result.predicted.func_names) - set(result.expected.func_names))\n",
    "        for result in evaluation_results\n",
    "    )\n",
    "    false_negatives = sum(\n",
    "        len(set(result.expected.func_names) - set(result.predicted.func_names))\n",
    "        for result in evaluation_results\n",
    "    )\n",
    "\n",
    "    precision = (\n",
    "        true_positives / (true_positives + false_positives)\n",
    "        if (true_positives + false_positives) > 0\n",
    "        else 0\n",
    "    )\n",
    "    recall = (\n",
    "        true_positives / (true_positives + false_negatives)\n",
    "        if (true_positives + false_negatives) > 0\n",
    "        else 0\n",
    "    )\n",
    "\n",
    "    return precision, recall\n",
    "\n",
    "\n",
    "precision, recall = calculate_precision_recall(evaluation_results)\n",
    "\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cadquery",
   "language": "python",
   "name": "cadquery"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
